{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lovrorabuzin/densenet-zavrad?scriptVersionId=91691855\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Importi i hiperparametri","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional\nimport torch.nn.functional as F\n\nimport sklearn\nfrom sklearn import model_selection\n\nimport torchvision\nimport torchvision.transforms as torch_transforms\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torchvision.models as models\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import random_split\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import Subset\nimport torch.utils.checkpoint as cp\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\n\nimport pickle\nimport numpy as np\nimport math\nimport pandas as pd\n\nimport skimage as ski\nimport skimage.io\nimport os\n\nimport random\n\nimport nibabel as nib\nfrom PIL import Image\nimport imageio\n\nimport sys\nimport json\n\nfrom math import e\n\n#Batch size je 1 jer sam morao radit mini-grupe dok sam ucitavao podatke zbog nekih specificnosti u radu s datasetovima na Kaggleu\nb_s = 1\n#Hiperparametri za promjenu stope ucenja\nlearning_rate = 0.0005\nweight_decay = 0.0005\nscaling_factor = 2\ngamma = 0.95\n#Otprilike koliko epoha mi je stalo u 9 sati rada u Kaggle kernelu\nnum_epochs = 24\n#Broj krisaka na kojima se trenira odjednom, to jest batch size\nslice_no = 200\nrandom.seed()\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nroot_train_dir = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":2.436211,"end_time":"2021-06-09T07:56:55.612964","exception":false,"start_time":"2021-06-09T07:56:53.176753","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-11T20:32:51.94355Z","iopub.execute_input":"2021-12-11T20:32:51.943923Z","iopub.status.idle":"2021-12-11T20:32:55.070097Z","shell.execute_reply.started":"2021-12-11T20:32:51.943844Z","shell.execute_reply":"2021-12-11T20:32:55.068755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Učitavanje podataka","metadata":{}},{"cell_type":"code","source":"name_mapping = pd.read_csv(root_train_dir + '/name_mapping.csv')\n\nname_mapping.rename({'BraTS_2020_subject_ID': 'ID'}, axis=1, inplace=True)\n\nsurvival_info = pd.read_csv(root_train_dir + '/survival_info.csv')\n\nsurvival_info.rename({'Brats20ID': 'ID'}, axis=1, inplace=True)\n\npatient_info = survival_info.merge(name_mapping, on=\"ID\", how=\"right\")\n\n#Sufiksi za nacine snimanja, koristeno pri ucitavanju\nmodalities = ['_flair.nii', '_t1.nii', '_t1ce.nii', '_t2.nii']\nmask_path = '_seg.nii'\n\ntrain_scan_files = []\nvalid_scan_files = []\ntest_scan_files = []\n\nHGG_names = list(patient_info[patient_info['Grade'] == 'HGG'].ID)\nLGG_names = list(patient_info[patient_info['Grade'] == 'LGG'].ID)\n\n#Moj dosta blesavi pristup za train/valid/test splittanje, primijeti da su razdijeljeni pacijenti s visokim stupnjem glioma (HGG)\n#i niskim stupnjem (LGG) da bi bili proporcionalno zastupljeni u train/valid/test skupovima\nfor i in range(len(HGG_names)):\n    image_path = root_train_dir+'/'+HGG_names[i]+'/'+HGG_names[i]\n    if i % 7 == 5:\n        valid_scan_files.append(image_path)\n    elif i % 7 == 4:\n        test_scan_files.append(image_path)\n    else:\n        train_scan_files.append(image_path)\n\nfor i in range(len(LGG_names)):\n    image_path = root_train_dir+'/'+LGG_names[i]+'/'+LGG_names[i]\n    if i % 7 == 5:\n        valid_scan_files.append(image_path)\n    elif i % 7 == 4:\n        test_scan_files.append(image_path)\n    else:\n        train_scan_files.append(image_path)\n\n        \n#Pri treniranju slike nasumicno zrcalim - daje bolje performanse\nhorizontalFlipTransform = torch_transforms.RandomHorizontalFlip(p = 0.5)\ntoTensor = torch_transforms.ToTensor()\n\ntrainTransform = torch_transforms.Compose([toTensor, horizontalFlipTransform])\n\n#Svaki volumen pojedinacno normaliziram da ima srednju vrijednost 0 i std 1\ndef normalize_vol(volume, i):\n    #pritom uzimam u obzir samo piksele koji pripadaju mozgu, ne pozadinu slike\n    logical_mask = volume != 0.\n    mean = np.mean(volume[logical_mask])\n    std = np.std(volume[logical_mask])\n    return (volume-mean)/std\n\nclass SegmentationDataset(Dataset):\n    in_channels = 4\n    out_channels = 4\n    \n    def __init__(self, paths, modalities, mask_path, tumor_slices, slice_no = 155, transform = None):\n        self.paths = paths\n        self.modalities = modalities\n        self.mask_path = mask_path\n        self.transform = transform\n        self.mask_buffer = np.array([])\n        self.volume_buffer = np.array([])\n        self.passes = 0\n        self.tumor_slices = tumor_slices\n        random.shuffle(self.paths)\n        \n    def __len__(self):\n        return math.ceil(self.tumor_slices/slice_no)\n    \n    def reset(self):\n        self.passes = 0\n        random.shuffle(self.paths)\n    \n    def __getitem__(self, idx):\n        if self.mask_buffer.size > 0:\n            bound = min(slice_no, self.mask_buffer.shape[0])\n            res_mask = self.mask_buffer[:bound]\n            res_volume = self.volume_buffer[:bound]\n            if bound < self.mask_buffer.shape[0]:\n                self.mask_buffer = self.mask_buffer[bound:]\n                self.volume_buffer = self.volume_buffer[bound:]\n            else:\n                self.mask_buffer = np.array([])\n                self.volume_buffer = np.array([])\n        else:\n            res_mask = np.array([])\n            res_volume = np.array([])\n        \n        while res_mask.shape[0] < slice_no and idx + self.passes < len(self.paths):\n            patient = self.paths[idx+self.passes]\n            self.passes += 1\n            volumes = []\n            single_mask = patient + mask_path\n            #Jedna segmentacijska mapa je krivo imenovana u datasetu, a buduci da dataset nije moj, ne mogu to popraviti tamo pa u kodu moram \n            if single_mask == root_train_dir + '/BraTS20_Training_355/BraTS20_Training_355_seg.nii':\n                single_mask = root_train_dir + '/BraTS20_Training_355/W39_1998.09.19_Segm.nii'\n            single_mask = nib.load(single_mask)\n            single_mask = np.asarray(single_mask.dataobj, dtype = np.float)\n            mask = single_mask.transpose(2,0,1)\n            mask_WT = mask.copy()\n            mask_WT[mask_WT == 1] = 0\n            mask_WT[mask_WT == 2] = 1\n            mask_WT[mask_WT == 4] = 0\n\n            mask_TC = mask.copy()\n            mask_TC[mask_TC == 1] = 1\n            mask_TC[mask_TC == 2] = 0\n            mask_TC[mask_TC == 4] = 0\n\n            mask_ET = mask.copy()\n            mask_ET[mask_ET == 1] = 0\n            mask_ET[mask_ET == 2] = 0\n            mask_ET[mask_ET == 4] = 1\n\n            mask_BG = mask.copy()\n            mask_BG[mask_BG == 0] = 3\n            mask_BG[mask_BG == 1] = 0\n            mask_BG[mask_BG == 2] = 0\n            mask_BG[mask_BG == 4] = 0\n            mask_BG[mask_BG == 3] = 1\n\n            mask_full = np.stack([mask_WT, mask_TC, mask_ET, mask_BG])\n            mask_full = np.transpose(mask_full, (1,0,2,3))\n\n            tumor_indices = []\n            mask_pure = []\n            for i in range(155):\n                mask_slice = mask_full[i]\n                if np.sum(mask_slice[0:3]) != 0:\n                    tumor_indices.append(i)\n                    mask_pure.append(mask_slice)\n            mask = np.stack(mask_pure)\n\n            for modality in modalities:\n                single_mod_volume = patient + modality\n                single_mod_volume = nib.load(single_mod_volume)\n                single_mod_volume = np.asarray(single_mod_volume.dataobj, dtype = np.float)\n                single_mod_volume = single_mod_volume.transpose(2,0,1)\n                intermittent = []\n                for i_s in tumor_indices:\n                    intermittent.append(single_mod_volume[i_s])\n                volumes.append(np.stack(intermittent))\n\n            if self.transform:\n                seed = random.randint(0,2**32)\n                random.seed(seed)\n                torch.manual_seed(seed)\n                volumes = np.transpose(np.stack(volumes), (1,0,2,3))\n                volumes = np.stack([self.transform(np.transpose(volumes[i], (1,2,0))) for i in range(np.shape(volumes)[0])])\n                volumes = np.transpose(np.stack(volumes), (1,0,2,3))\n                random.seed(seed)\n                torch.manual_seed(seed)\n                mask = np.stack([self.transform(np.transpose(mask[i], (1,2,0))) for i in range(np.shape(mask)[0])])\n            volumes = [normalize_vol(volumes[i], i) for i in range(np.shape(volumes)[0])]\n            volumes = np.transpose(np.stack(volumes), (1,0,2,3))\n            bound = min(slice_no-res_mask.shape[0], mask.shape[0])\n            if res_mask.size == 0:\n                res_mask = mask[:bound]\n                res_volume = volumes[:bound]\n            else:\n                res_mask = np.concatenate((res_mask, mask[:bound]), axis = 0)\n                res_volume = np.concatenate((res_volume, volumes[:bound]), axis = 0)\n            if bound < mask.shape[0]:\n                if self.mask_buffer.size == 0:\n                    self.mask_buffer = mask[bound:]\n                    self.volume_buffer = volumes[bound:]\n                else:\n                    self.mask_buffer = np.concatenate((self.mask_buffer, mask[bound:]), axis = 0)\n                    self.volume_buffer = np.concatenate((self.volume_buffer, volumes[bound:]), axis = 0)\n        self.passes -= 1\n        res_volume = torch.from_numpy(res_volume).float()\n        res_mask = torch.from_numpy(res_mask).long()\n        return res_volume, res_mask\n    \nclass TestingDataset(Dataset):\n    in_channels = 4\n    out_channels = 4\n    \n    def __init__(self, paths, modalities, mask_path):\n        self.paths = paths\n        self.modalities = modalities\n        self.mask_path = mask_path\n        self.mask_buffer = np.array([])\n        self.volume_buffer = np.array([])\n        \n        \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, idx):\n        patient = self.paths[idx]\n        volumes = []\n        single_mask = patient + mask_path\n        if single_mask == root_train_dir + '/BraTS20_Training_355/BraTS20_Training_355_seg.nii':\n            single_mask = root_train_dir + '/BraTS20_Training_355/W39_1998.09.19_Segm.nii'\n        single_mask = nib.load(single_mask)\n        single_mask = np.asarray(single_mask.dataobj, dtype = np.float)\n        mask = single_mask.transpose(2,0,1)\n        mask_WT = mask.copy()\n        mask_WT[mask_WT == 1] = 0\n        mask_WT[mask_WT == 2] = 1\n        mask_WT[mask_WT == 4] = 0\n\n        mask_TC = mask.copy()\n        mask_TC[mask_TC == 1] = 1\n        mask_TC[mask_TC == 2] = 0\n        mask_TC[mask_TC == 4] = 0\n\n        mask_ET = mask.copy()\n        mask_ET[mask_ET == 1] = 0\n        mask_ET[mask_ET == 2] = 0\n        mask_ET[mask_ET == 4] = 1\n\n        mask_BG = mask.copy()\n        mask_BG[mask_BG == 0] = 3\n        mask_BG[mask_BG == 1] = 0\n        mask_BG[mask_BG == 2] = 0\n        mask_BG[mask_BG == 4] = 0\n        mask_BG[mask_BG == 3] = 1\n        mask_full = np.stack([mask_WT, mask_TC, mask_ET, mask_BG])#, mask_BG\n        mask_full = np.transpose(mask_full, (1,0,2,3))\n        \n        for modality in modalities:\n                single_mod_volume = patient + modality\n                single_mod_volume = nib.load(single_mod_volume)\n                single_mod_volume = np.asarray(single_mod_volume.dataobj, dtype = np.float)\n                single_mod_volume = single_mod_volume.transpose(2,0,1)\n                volumes.append(np.stack(single_mod_volume))\n\n        volumes = [normalize_vol(volumes[i], i) for i in range(np.shape(volumes)[0])]\n        volumes = np.transpose(np.stack(volumes), (1,0,2,3))\n        res_volume = torch.from_numpy(volumes).float()\n        res_mask = torch.from_numpy(mask_full).long()\n\n        return res_volume, res_mask\n\n\n\ntrainset = SegmentationDataset(train_scan_files, modalities, mask_path, 17227, transform = trainTransform)\nvalidset = SegmentationDataset(valid_scan_files, modalities, mask_path, 3661)\ntestset = TestingDataset(test_scan_files, modalities, mask_path)\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=b_s, shuffle=False, num_workers=0)\nvalidloader = torch.utils.data.DataLoader(validset, batch_size=b_s,shuffle=False, num_workers=0)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=b_s, shuffle=False, num_workers=0)","metadata":{"papermill":{"duration":0.14091,"end_time":"2021-06-09T07:56:55.769385","exception":false,"start_time":"2021-06-09T07:56:55.628475","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-11T20:33:04.294865Z","iopub.execute_input":"2021-12-11T20:33:04.295209Z","iopub.status.idle":"2021-12-11T20:33:04.413056Z","shell.execute_reply.started":"2021-12-11T20:33:04.29518Z","shell.execute_reply":"2021-12-11T20:33:04.412023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neuronska mreža (Ladder Densenet)","metadata":{}},{"cell_type":"code","source":"upsample = lambda x, size: F.interpolate(x, size, mode='bilinear', align_corners=False)\ncheckpoint = lambda func, *inputs: cp.checkpoint(func, *inputs, preserve_rng_state=True)\nbatchnorm_momentum = 0.05\nbatchnorm_momentum_ckpt = min(np.roots([-1, 2, -batchnorm_momentum]))\nuse_batchnorm = True\navg_pooling_k = 2\nuse_dws_up = False\nuse_dws_down = False\ncheckpoint_stem = True\nuse_pyl_in_spp = True\ncheckpoint_upsample = False\n\ndef get_pyramid_loss_scales(downsampling_factor, upsampling_factor):\n    num_scales = int(math.log2(downsampling_factor // upsampling_factor))\n    scales = [downsampling_factor]\n    for i in range(num_scales - 1):\n        assert scales[-1] % 2 == 0\n        scales.append(scales[-1] // 2)\n    return scales\n\ndef _batchnorm_factory(num_maps, momentum):\n    return nn.BatchNorm2d(num_maps, eps=1e-5, momentum=momentum)\n\ndef _checkpoint_unit_nobt(bn, relu, conv):\n    def func(*x):\n        x = torch.cat(x, 1)\n        return conv(relu(bn(x)))\n    return func\n\ndef _checkpoint_unit(bn1, relu1, conv1, bn2, relu2, conv2):\n    def func(*x):\n        x = torch.cat(x, 1)\n        x = conv1(relu1(bn1(x)))\n        return conv2(relu2(bn2(x)))\n    return func\n\nclass _Transition(nn.Sequential):\n    @staticmethod\n    def _checkpoint_function(bn, relu, conv, pool):\n        def func(inputs):\n            return pool(conv(relu(bn(inputs))))\n        return func\n\n    def __init__(self, num_input_features, num_output_features, stride=2, checkpointing=False):\n        super(_Transition, self).__init__()\n        self.stride = stride\n        if use_batchnorm:\n            m = batchnorm_momentum_ckpt if checkpointing else batchnorm_momentum\n            self.add_module('norm', _batchnorm_factory(num_input_features, m))\n        self.add_module('relu', nn.ReLU(inplace=True))\n        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        if stride > 1:\n            if avg_pooling_k == 2:\n                self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=stride))\n            elif avg_pooling_k == 3:\n                self.add_module('pool', nn.AvgPool2d(kernel_size=3, stride=stride,\n            \t\t\t\t\tpadding=1, ceil_mode=False, count_include_pad=False))\n        else:\n            self.pool = lambda x: x\n        self.checkpointing = checkpointing\n        if checkpointing:\n            self.func = _Transition._checkpoint_function(self.norm, self.relu, self.conv, self.pool)\n\n    def forward(self, x):\n        if self.checkpointing and self.training:\n            return checkpoint(self.func, x)\n        else:\n            return super(_Transition, self).forward(x)\n\nclass _BNReluConv(nn.Sequential):\n    @staticmethod\n    def _checkpoint_function(bn, relu, conv):\n        def func(inputs):\n            return conv(relu(bn(inputs)))\n        return func\n\n    def __init__(self, num_maps_in, num_maps_out, k=3, output_conv=False,\n                 dilation=1, drop_rate=0, checkpointing=False):\n        super(_BNReluConv, self).__init__()\n        self.drop_rate = drop_rate\n        if use_batchnorm:\n            m = batchnorm_momentum_ckpt if checkpointing else batchnorm_momentum\n            self.add_module('norm', _batchnorm_factory(num_maps_in, m))\n        self.add_module('relu', nn.ReLU(inplace=True))\n        padding = ((k-1) // 2) * dilation\n        if k >= 3 and use_dws_up:\n            self.add_module('conv', SeparableConv2d(num_maps_in, num_maps_out, kernel_size=k,\n                padding=padding, bias=output_conv, dilation=dilation))\n        else:\n            self.add_module('conv', nn.Conv2d(num_maps_in, num_maps_out, kernel_size=k,\n                            padding=padding, bias=output_conv, dilation=dilation))\n        self.checkpointing = checkpointing\n        if checkpointing:\n            self.func = _BNReluConv._checkpoint_function(self.norm, self.relu, self.conv)\n\n    def forward(self, x):\n        if self.checkpointing and self.training:\n            x = checkpoint(self.func, x)\n        else:\n            x = super(_BNReluConv, self).forward(x)\n        return x\n\nclass SpatialPyramidPooling(nn.Module):\n    def __init__(self, conv_class, upsample_func, num_maps_in, bt_size=512, level_size=128,\n                 out_size=256, grids=[6,3,2,1], square_grid=False):\n        super(SpatialPyramidPooling, self).__init__()\n        self.upsample = upsample_func\n        self.grids = grids\n        self.num_levels = len(grids)\n        self.square_grid = square_grid\n        self.spp = nn.Sequential()\n        self.spp.add_module('spp_bn', conv_class(num_maps_in, bt_size, k=1))\n        num_features = bt_size\n        final_size = num_features\n        for i in range(self.num_levels):\n            final_size += level_size\n            self.spp.add_module('spp'+str(i), conv_class(num_features, level_size, k=1))\n        self.spp.add_module('spp_fuse', conv_class(final_size, out_size, k=1))\n\n    def forward(self, x):\n        levels = []\n        target_size = x.size()[2:4]\n        ar = target_size[1] / target_size[0]\n        x = self.spp[0].forward(x)\n        levels.append(x)\n        num = len(self.spp) - 1\n        for i in range(1, num):\n            if not self.square_grid:\n                grid_size = (self.grids[i-1], max(1, round(ar*self.grids[i-1])))\n                x_pooled = F.adaptive_avg_pool2d(x, grid_size)\n            else:\n                x_pooled = F.adaptive_avg_pool2d(x, self.grids[i-1])\n            level = self.spp[i].forward(x_pooled)\n            level = self.upsample(level, target_size)\n            levels.append(level)\n        x = torch.cat(levels, 1)\n        return self.spp[-1].forward(x)\n\nclass UpsampleResidual(nn.Module):\n    def __init__(self, conv_class, upsample_func, num_maps_in, skip_maps_in, num_maps_out, k,\n                 produce_aux=False, num_classes=0, dws_conv=False):\n        super(UpsampleResidual, self).__init__()\n        self.upsample_func = upsample_func\n        self.bottleneck = conv_class(skip_maps_in, num_maps_in, k=1)\n        self.produce_aux = produce_aux\n        self.has_blend_conv = num_maps_out > 0\n        self.num_maps_out = num_maps_in\n        if num_maps_out != num_maps_in:\n            self.skip_bt = conv_class(num_maps_in, num_maps_out, k=1)\n        else:\n            self.skip_bt = None\n        if produce_aux:\n            self.aux_logits = conv_class(num_maps_in, num_classes, k=1, output_conv=True)\n        if self.has_blend_conv:\n            self.num_maps_out = num_maps_out\n            bt_maps = 128\n            self.blend_bt = None\n            if not dws_conv and k >=3 and num_maps_in > bt_maps:\n                self.blend_bt = conv_class(num_maps_in, bt_maps, k=1)\n                num_maps_in = bt_maps\n            self.blend_conv = conv_class(num_maps_in, num_maps_out, k=k)\n\n    def forward(self, bottom, skip):\n        skip = self.bottleneck(skip)\n        skip_size = skip.size()[2:4]\n        if self.produce_aux:\n            aux = self.aux_logits(bottom)\n\n        bottom = self.upsample_func(bottom, skip_size)\n        x = skip\n        x += bottom\n\n        if self.has_blend_conv:\n            if self.blend_bt is not None:\n                x = self.blend_bt(x)\n            x = self.blend_conv(x)\n        if self.skip_bt is not None:\n            bottom = self.skip_bt(bottom)\n        x += bottom\n        if self.produce_aux:\n            return x, aux\n        return x\n\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate,\n                 dilation=1, checkpointing=True):\n        super(_DenseLayer, self).__init__()\n        m = batchnorm_momentum_ckpt if checkpointing else batchnorm_momentum\n        bottleneck_size = bn_size * growth_rate\n        self.add_module('norm1', _batchnorm_factory(num_input_features, m))\n        self.add_module('relu1', nn.ReLU(inplace=True)),\n        self.add_module('conv1', nn.Conv2d(num_input_features, bottleneck_size,\n                        kernel_size=1, stride=1, bias=False))\n        num_feats = bottleneck_size\n        self.add_module('norm2', _batchnorm_factory(num_feats, m))\n        self.add_module('relu2', nn.ReLU(inplace=True)),\n        self.add_module('conv2', nn.Conv2d(num_feats, growth_rate, kernel_size=3,\n                            stride=1, padding=dilation, bias=False, dilation=dilation))\n        self.drop_rate = drop_rate\n        self.checkpointing = checkpointing\n        if checkpointing:\n            if len(self) == 6:\n                self.conv_func = _checkpoint_unit(self.norm1, self.relu1, self.conv1,\n                                                  self.norm2, self.relu2, self.conv2)\n            else:\n                self.conv_func = _checkpoint_unit_nobt(self.norm2, self.relu2, self.conv2)\n\n    def forward(self, x):\n        if self.checkpointing:\n            if self.training:\n                x = checkpoint(self.conv_func, *x)\n            else:\n                x = self.conv_func(*x)\n        else:\n            x = super(_DenseLayer, self).forward(x)\n\n        if self.drop_rate > 0:\n            x = F.dropout(x, p=self.drop_rate, training=self.training, inplace=True)\n\n        return x\n\n\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate,\n                 split=False, dilation=1, checkpointing=True):\n        super(_DenseBlock, self).__init__()\n        self.checkpointing = checkpointing\n        self.split = split\n\n        for i in range(num_layers):\n            layer = _DenseLayer(\n                num_input_features + i * growth_rate, growth_rate=growth_rate, bn_size=bn_size,\n                drop_rate=drop_rate, dilation=dilation, checkpointing=checkpointing)\n            self.add_module('denselayer%d' % (i + 1), layer)\n        if split:\n            self.split_size = num_input_features + (num_layers // 2) * growth_rate\n            k = avg_pooling_k\n            pad = (k-1) // 2\n            self.pool_func = lambda x: F.avg_pool2d(x, k, 2, padding=pad, ceil_mode=False,\n                                                    count_include_pad=False)\n\n    def forward(self, x):\n        if self.checkpointing:\n            x = [x]\n        for i, layer in enumerate(self.children()):\n            if self.split and len(self) // 2 == i:\n                if self.checkpointing:\n                    split = torch.cat(x, 1)\n                    x = [self.pool_func(split)]\n                else:\n                    split = x\n                    x = self.pool_func(split)\n            if self.checkpointing:\n                x.append(layer(x))\n            else:\n                x = torch.cat([x, layer(x)], 1)\n        if self.checkpointing:\n            x = torch.cat(x, 1)\n        if self.split:\n            return x, split\n        else:\n            return x\n\nclass DenseNet(nn.Module):\n    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n                 num_init_features=64, bn_size=4, drop_rate=0):\n        super(DenseNet, self).__init__()\n        batchnorm_momentum_ckpt = min(np.roots([-1, 2, -batchnorm_momentum]))\n        self.first_stride = 2\n        self.num_classes = 4\n        self.num_logits = 4\n        self.checkpointing = True\n        self.growth_rate = growth_rate\n        self.block_config = block_config\n        self.num_blocks = len(block_config)\n        self.growth_rate = growth_rate\n        self.features = nn.Sequential()\n        self.checkpoint_stem = True\n        self.features.add_module('conv0', nn.Conv2d(4, num_init_features, kernel_size=7,\n                                 stride=self.first_stride, padding=3, bias=False))\n        m = batchnorm_momentum_ckpt\n        self.features.add_module('norm0', _batchnorm_factory(num_init_features, m))\n        self.features.add_module('relu0', nn.ReLU(inplace=True))\n        self.features.add_module('pool0', nn.MaxPool2d(kernel_size=2, stride=2))\n        self.first_block_idx = len(self.features)\n        self.first_ckpt_func = self._checkpoint_segment(0, self.first_block_idx)\n        self.random_init = []\n        self.fine_tune = []\n        self.fine_tune.append(self.features)\n        splits = [False, False, False, False]\n        up_sizes = [256, 256, 128]\n        spp_square_grid = False\n        spp_grids = [8,4,2,1]\n        self.spp_size = 512\n        bt_size = 512\n        level_size = 128\n        dilations = [1] * len(block_config)\n        strides = [2] * (len(block_config) - 1)\n        num_downs = self.first_stride + strides.count(2) + sum(splits)\n        num_ups = len(up_sizes)\n        self.downsampling_factor = 2**num_downs\n        self.upsampling_factor = 2**(num_downs-num_ups)\n        self.use_upsampling_path = True\n        skip_sizes = []\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(\n                num_layers=num_layers, num_input_features=num_features, bn_size=bn_size,\n                growth_rate=growth_rate, drop_rate=0, split=splits[i],\n                dilation=dilations[i], checkpointing=self.checkpointing)\n            self.features.add_module('denseblock%d' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if block.split and self.use_upsampling_path:\n                skip_sizes.append(block.split_size)\n            if i != len(block_config) - 1:\n                if strides[i] > 1 and self.use_upsampling_path:\n                    skip_sizes.append(num_features)\n                trans = _Transition(\n                    num_input_features=num_features, num_output_features=num_features // 2,\n                    stride=strides[i], checkpointing=self.checkpointing)\n                self.features.add_module('transition%d' % (i + 1), trans)\n                num_features = num_features // 2\n        self.use_aux = False\n        self.spp = SpatialPyramidPooling(\n            _BNReluConv, upsample, num_features, bt_size, level_size,\n            self.spp_size, spp_grids, spp_square_grid)\n        self.random_init.append(self.spp)\n        num_features = self.spp_size\n        if self.use_aux:\n            self.pyramid_loss_scales = get_pyramid_loss_scales(\n                    args.downsampling_factor, args.upsampling_factor)\n            spp_scales = []\n            for scale in reversed(spp_grids):\n                assert args.crop_size % scale == 0\n                spp_scales.append(args.crop_size // scale)\n            self.pyramid_loss_scales = spp_scales + self.pyramid_loss_scales\n        if self.use_upsampling_path:\n            up_convs = [3] * len(up_sizes)\n            self.upsample_layers = nn.Sequential()\n            self.random_init.append(self.upsample_layers)\n            assert len(up_sizes) == len(skip_sizes)\n            for i in range(num_ups):\n                upsample_unit = UpsampleResidual(\n                    _BNReluConv, upsample, num_features, skip_sizes[-1-i], up_sizes[i],\n                    up_convs[i], False, self.num_classes)\n                num_features = upsample_unit.num_maps_out\n                self.upsample_layers.add_module('upsample_'+str(i), upsample_unit)\n        self.logits = _BNReluConv(num_features, self.num_logits, k=1, output_conv=True,\n                                  checkpointing=self.checkpointing)\n        self.random_init.append(self.logits)\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        nn.init.xavier_normal_(self.logits.conv.weight.data)\n        if self.use_aux and self.use_upsampling_path:\n            for module in self.upsample_layers:\n                nn.init.xavier_normal_(module.aux_logits.conv.weight.data)\n\n    \n    def forward(self, x, target_size=None):\n        skip_layers = []\n        if target_size is None:\n            target_size = x.size()[2:4]\n\n        if not self.training or not self.checkpoint_stem:\n            for i in range(self.first_block_idx+1):\n                x = self.features[i].forward(x)\n        else:\n            x = checkpoint(self.first_ckpt_func, x)\n            x = self.features[self.first_block_idx].forward(x)\n        for i in range(self.first_block_idx+1, len(self.features), 2):\n            if self.features[i].stride > 1 and self.use_upsampling_path:\n                skip_layers.append(x)\n            x = self.features[i].forward(x)\n            x = self.features[i+1].forward(x)\n            if isinstance(x, tuple) and self.use_upsampling_path:\n                x, split = x\n                skip_layers.append(split)\n\n        x = self.spp(x)\n\n        aux_logits = []\n        if self.use_upsampling_path:\n            for i, up in enumerate(self.upsample_layers):\n                x = up(x, skip_layers[-1-i])\n                if self.use_aux:\n                    x, aux = x\n                    aux_logits.append(aux)\n\n        x = self.logits(x)\n        x = upsample(x, target_size)\n\n        return x, aux_logits\n    \n    def _checkpoint_segment(self, start, end):\n        def func(x):\n            for i in range(start, end):\n                x = self.features[i](x)\n            return x\n        return func\n    \n    def forward_loss(self, batch, return_outputs=False):\n        x = batch['image']\n        logits, aux_logits = self.forward(x)\n        if not self.training:\n            aux_logits = []\n        self.output = logits\n        loss = losses.segmentation_loss(logits, aux_logits, batch, self.args.aux_loss_weight,\n                                        self.dataset.ignore_id, equal_level_weights=use_pyl_in_spp)\n        loss, self.aux_losses = loss\n        if return_outputs:\n            return loss, (logits, aux_logits)\n        return loss\n\nclass SemSegModel(nn.Module):\n    def __init__(self, backbone, num_classes):\n        super(SemSegModel, self).__init__()\n        self.backbone = backbone\n\n    def forward(self, image):\n        logits, additional = self.backbone(image)\n        nonlin = nn.LogSoftmax(1)\n        return nonlin(logits)","metadata":{"papermill":{"duration":0.146477,"end_time":"2021-06-09T07:56:55.965746","exception":false,"start_time":"2021-06-09T07:56:55.819269","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-11T20:33:10.449921Z","iopub.execute_input":"2021-12-11T20:33:10.450262Z","iopub.status.idle":"2021-12-11T20:33:10.544963Z","shell.execute_reply.started":"2021-12-11T20:33:10.450232Z","shell.execute_reply":"2021-12-11T20:33:10.543548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pomoćne funkcije","metadata":{}},{"cell_type":"code","source":"def dice_score(prediction, ground_truth, smooth = 1.0):\n    prediction = prediction.contiguous().view(-1)\n    ground_truth = ground_truth.contiguous().view(-1)\n    \n    intersection = (prediction*ground_truth).sum()\n    score = (2*intersection+smooth)/(prediction.sum()+ground_truth.sum()+smooth)\n    return score\n\ndef un_one_hot(targets):\n    return targets.argmax(1)\n\ndef output_metrics(loss_avg, dice_scores, phase):\n    print(phase)\n    print(\"Average loss: {}\".format(loss_avg))\n    print(\"Dice score: {}\".format(dice_scores))\n\ndef dataset_evaluate(model, loader, loss_fn, threshold = -1, single = False):\n    global device\n    \n    loss_avg = 0.0\n    dice_scores = []\n    \n    with torch.no_grad():\n        model.eval()\n        for images, ground_truths in loader:\n            images = torch.cat([images[i] for i in range(b_s)])\n            ground_truths = torch.cat([ground_truths[i] for i in range(b_s)])\n            images = images.to(device)\n            ground_truths = ground_truths.to(device)\n            outputs = model(images)\n            loss = loss_fn(outputs, un_one_hot(ground_truths))\n            loss_avg += loss.item()\n            outputs = outputs.argmax(1)\n            ground_truths = ground_truths.argmax(1)\n            outputs_WT = outputs.clone()\n            outputs_WT[outputs_WT == 0] = 1\n            outputs_WT[outputs_WT == 1] = 1\n            outputs_WT[outputs_WT == 2] = 1\n            outputs_WT[outputs_WT == 3] = 0\n            outputs_TC = outputs.clone()\n            outputs_TC[outputs_TC == 0] = 0\n            outputs_TC[outputs_TC == 1] = 1\n            outputs_TC[outputs_TC == 2] = 1\n            outputs_TC[outputs_TC == 3] = 0\n            outputs_ET = outputs.clone()\n            outputs_ET[outputs_ET == 0] = 0\n            outputs_ET[outputs_ET == 1] = 0\n            outputs_ET[outputs_ET == 2] = 1\n            outputs_ET[outputs_ET == 3] = 0\n            ground_truths_WT = ground_truths.clone()\n            ground_truths_WT[ground_truths_WT == 0] = 1\n            ground_truths_WT[ground_truths_WT == 1] = 1\n            ground_truths_WT[ground_truths_WT == 2] = 1\n            ground_truths_WT[ground_truths_WT == 3] = 0\n            ground_truths_TC = ground_truths.clone()\n            ground_truths_TC[ground_truths_TC == 0] = 0\n            ground_truths_TC[ground_truths_TC == 1] = 1\n            ground_truths_TC[ground_truths_TC == 2] = 1\n            ground_truths_TC[ground_truths_TC == 3] = 0\n            ground_truths_ET = ground_truths.clone()\n            ground_truths_ET[ground_truths_ET == 0] = 0\n            ground_truths_ET[ground_truths_ET == 1] = 0\n            ground_truths_ET[ground_truths_ET == 2] = 1\n            ground_truths_ET[ground_truths_ET == 3] = 0\n            \n            dice_scores.append([dice_score(outputs_WT, ground_truths_WT),\n                                dice_score(outputs_TC, ground_truths_TC),\n                                dice_score(outputs_ET, ground_truths_ET)])\n                \n    loss_avg /= len(loader)\n    model.train()\n    dice_scores = np.stack(dice_scores)\n    return loss_avg, dice_scores.mean(0)","metadata":{"papermill":{"duration":0.058976,"end_time":"2021-06-09T07:56:56.251459","exception":false,"start_time":"2021-06-09T07:56:56.192483","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-11T20:33:15.934Z","iopub.execute_input":"2021-12-11T20:33:15.934417Z","iopub.status.idle":"2021-12-11T20:33:15.954371Z","shell.execute_reply.started":"2021-12-11T20:33:15.934385Z","shell.execute_reply":"2021-12-11T20:33:15.953005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Iscrtavanje grafa gubitka","metadata":{}},{"cell_type":"code","source":"def plot_progress(data):\n    valid_loss = data['valid loss']\n    train_loss = data['train loss']\n\n    fig, ax = plt.subplots(figsize=(16,8))\n    linewidth = 2\n    legend_size = 10\n    train_color = 'm'\n    val_color = 'c'\n\n    ax.set_title('Loss')\n    ax.plot(train_loss, marker='o', color=train_color,\n           linewidth=linewidth, linestyle='-', label='train')\n    ax.plot(valid_loss, marker='o', color=val_color,\n           linewidth=linewidth, linestyle='-', label='validation')\n    ax.legend(loc='upper right', fontsize=legend_size)\n\n    save_path = os.path.join('./', 'loss.png')\n    print('Plotting in: ', save_path)\n    plt.savefig(save_path)\n    return","metadata":{"papermill":{"duration":0.039654,"end_time":"2021-06-09T07:56:56.315138","exception":false,"start_time":"2021-06-09T07:56:56.275484","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-11T20:33:31.433975Z","iopub.execute_input":"2021-12-11T20:33:31.434368Z","iopub.status.idle":"2021-12-11T20:33:31.450835Z","shell.execute_reply.started":"2021-12-11T20:33:31.434338Z","shell.execute_reply":"2021-12-11T20:33:31.449474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Treniranje i evaluacija modela","metadata":{}},{"cell_type":"code","source":"mydensenet = DenseNet(num_init_features = 64, growth_rate = 32, block_config = (6,12,24,16))\nnet = SemSegModel(mydensenet, 4)\nprint(net)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T20:33:45.292955Z","iopub.execute_input":"2021-12-11T20:33:45.293284Z","iopub.status.idle":"2021-12-11T20:33:45.622096Z","shell.execute_reply.started":"2021-12-11T20:33:45.293255Z","shell.execute_reply":"2021-12-11T20:33:45.621012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_network():\n    global device\n    plot_data = {}\n    plot_data[\"train loss\"] = []\n    plot_data[\"valid loss\"] = []\n    plot_data[\"WT valid dice\"] = []\n    plot_data[\"TC valid dice\"] = []\n    plot_data[\"ET valid dice\"] = []\n    plot_data[\"test loss\"] = 0\n    plot_data[\"WT test dice\"] = []\n    plot_data[\"TC test dice\"] = []\n    plot_data[\"ET test dice\"] = []\n    plot_data[\"lr\"] = []\n    \n    SAVE_PATH = \"./network.pt\"\n    print(\"device:\", device)\n    \n    densenet121 = models.densenet121(pretrained=True, memory_efficient = True)\n    mydensenet = DenseNet(num_init_features = 64, growth_rate = 32, block_config = (6,12,24,16))\n\n    pretrained_dict = densenet121.state_dict()\n    model_dict = mydensenet.state_dict()\n\n    \n    conv_weight = pretrained_dict.pop('features.conv0.weight')\n    mean_kernel = conv_weight.mean(dim=1)\n    mean_kernel = torch.unsqueeze(mean_kernel, dim=1)\n    mean_kernel = torch.cat([mean_kernel]*4, dim = 1)\n    pretrained_dict['features.conv0.weight'] = mean_kernel\n\n    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n    pretrained_names = list(pretrained_dict.keys())\n    \n    model_dict.update(pretrained_dict)\n\n    mydensenet.load_state_dict(model_dict)\n    \n    net = SemSegModel(mydensenet, 4).to(device = device)\n    lossFunc = nn.NLLLoss(weight = torch.tensor([2,5,3,1]).float().to(device))\n    \n    base_params = [v[1] for v in list(filter(lambda kv: kv[0] not in pretrained_names, net.named_parameters()))]\n    pretrained_params = [v[1] for v in list(filter(lambda kv: kv[0] in pretrained_names, net.named_parameters()))]\n    \n    optimizer = optim.Adam([{'params': base_params}, {'params': pretrained_params, 'lr':learning_rate/scaling_factor}], lr=learning_rate, weight_decay = weight_decay)\n    \n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = num_epochs, eta_min = 1e-6)\n    \n    for e in range(num_epochs):\n        accLoss = 0.\n        net.train()\n        i = -1\n        for inputs, ground_truths in trainloader:\n            i+=1\n            inputs = torch.cat([inputs[i] for i in range(b_s)])\n            ground_truths = torch.cat([ground_truths[i] for i in range(b_s)])\n            inputs.requires_grad = True\n            inputs = inputs.to(device)\n            ground_truths = ground_truths.to(device)\n            \n            outputs = net(inputs)\n            loss = lossFunc(outputs, un_one_hot(ground_truths))\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            accLoss += loss.item()\n\n            if i % 10 == 0:\n                print(\"Epoch: %d, Iteration: %5d, Loss: %.3f\" % ((e + 1), (i+1), (accLoss / (i + 1))))\n        trainset.reset()\n\n        val_loss, val_dice = dataset_evaluate(net, validloader, lossFunc)\n        validset.reset()\n        output_metrics(val_loss, val_dice, \"Validation\")\n        plot_data[\"valid loss\"].append(val_loss)\n        plot_data[\"WT valid dice\"].append(val_dice[0])\n        plot_data[\"TC valid dice\"].append(val_dice[1])\n        plot_data[\"ET valid dice\"].append(val_dice[2])\n        \n        plot_data[\"lr\"].append(scheduler.get_last_lr())\n        plot_data[\"train loss\"].append(accLoss/(i+1))\n        \n        scheduler.step()\n    \n    test_loss, test_dice = dataset_evaluate(net, testloader, lossFunc)\n    output_metrics(test_loss, test_dice, \"Test:\")\n    \n    plot_data[\"test loss\"] = test_loss\n    plot_data[\"WT test dice\"] = test_dice[0]\n    plot_data[\"TC test dice\"] = test_dice[1]\n    plot_data[\"ET test dice\"] = test_dice[2]\n    \n    torch.save(net.state_dict(), SAVE_PATH)\n    \n    with open(\"./epoch_data.txt\", 'w') as f:\n        f.write(repr(plot_data))\n\n    plot_progress(plot_data)\n\n    return plot_data\n    \n    \nepoch_data = train_network()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T20:34:11.258781Z","iopub.execute_input":"2021-12-11T20:34:11.259235Z","iopub.status.idle":"2021-12-11T23:45:59.943614Z","shell.execute_reply.started":"2021-12-11T20:34:11.259206Z","shell.execute_reply":"2021-12-11T23:45:59.940492Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
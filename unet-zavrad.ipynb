{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lovrorabuzin/unet-zavrad?scriptVersionId=91691982\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Importi i hiperparametri","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional\nimport torch.nn.functional as F\n\nimport sklearn\nfrom sklearn import model_selection\n\nimport torchvision\nimport torchvision.transforms as torch_transforms\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torchvision.models as models\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import random_split\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import Subset\nimport torch.utils.checkpoint as cp\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\n\nimport pickle\nimport numpy as np\nimport math\nimport pandas as pd\n\nimport skimage as ski\nimport skimage.io\nimport os\n\nimport random\n\nimport nibabel as nib\nfrom PIL import Image\nimport imageio\n\nimport torch.utils.checkpoint as cp\n\nimport sys\nimport json\n\nb_s = 1\nlearning_rate = 0.0001\nweight_decay = 0.0005\nscaling_factor = 2 # faktor smanjivanja koraka učenja za slojeve inicijalizirane predtreniranim parametrima\ngamma = 0.95\nnum_epochs = 30\nslice_no = 100\nrandom.seed()\nroot_train_dir = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":2.907136,"end_time":"2021-06-07T11:01:29.276733","exception":false,"start_time":"2021-06-07T11:01:26.369597","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T20:57:30.327134Z","iopub.execute_input":"2021-06-10T20:57:30.327539Z","iopub.status.idle":"2021-06-10T20:57:30.340679Z","shell.execute_reply.started":"2021-06-10T20:57:30.327492Z","shell.execute_reply":"2021-06-10T20:57:30.339507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Učitavanje podataka","metadata":{}},{"cell_type":"code","source":"name_mapping = pd.read_csv(root_train_dir + '/name_mapping.csv')\n\nname_mapping.rename({'BraTS_2020_subject_ID': 'ID'}, axis=1, inplace=True)\n\nsurvival_info = pd.read_csv(root_train_dir + '/survival_info.csv')\n\nsurvival_info.rename({'Brats20ID': 'ID'}, axis=1, inplace=True)\n\npatient_info = survival_info.merge(name_mapping, on=\"ID\", how=\"right\")\n\nmodalities = ['_flair.nii', '_t1.nii', '_t1ce.nii', '_t2.nii']\nmask_path = '_seg.nii'\n\ntrain_scan_files = []\nvalid_scan_files = []\ntest_scan_files = []\n\nHGG_names = list(patient_info[patient_info['Grade'] == 'HGG'].ID)\nLGG_names = list(patient_info[patient_info['Grade'] == 'LGG'].ID)\n\nfor i in range(len(HGG_names)):\n    image_path = root_train_dir+'/'+HGG_names[i]+'/'+HGG_names[i]\n    if i % 7 == 5:\n        valid_scan_files.append(image_path)\n    elif i % 7 == 4:\n        test_scan_files.append(image_path)\n    else:\n        train_scan_files.append(image_path)\n\nfor i in range(len(LGG_names)):\n    image_path = root_train_dir+'/'+LGG_names[i]+'/'+LGG_names[i]\n    if i % 7 == 5:\n        valid_scan_files.append(image_path)\n    elif i % 7 == 4:\n        test_scan_files.append(image_path)\n    else:\n        train_scan_files.append(image_path)\n\nhorizontalFlipTransform = torch_transforms.RandomHorizontalFlip(p = 0.5)\ntoTensor = torch_transforms.ToTensor()\n\ntrainTransform = torch_transforms.Compose([toTensor, horizontalFlipTransform])\n\ndef normalize_vol(volume, i):\n    logical_mask = volume != 0.\n    mean = np.mean(volume[logical_mask])\n    std = np.std(volume[logical_mask])\n    return (volume-mean)/std\n\nclass SegmentationDataset(Dataset):\n    in_channels = 4\n    out_channels = 4\n    \n    def __init__(self, paths, modalities, mask_path, tumor_slices, slice_no = 155, transform = None):\n        self.paths = paths\n        self.modalities = modalities\n        self.mask_path = mask_path\n        self.transform = transform\n        self.mask_buffer = np.array([])\n        self.volume_buffer = np.array([])\n        self.passes = 0\n        self.tumor_slices = tumor_slices\n        random.shuffle(self.paths)\n        \n    def __len__(self):\n        return math.ceil(self.tumor_slices/slice_no)\n    \n    def reset(self):\n        self.passes = 0\n        random.shuffle(self.paths)\n    \n    def __getitem__(self, idx):\n        if self.mask_buffer.size > 0:\n            bound = min(slice_no, self.mask_buffer.shape[0])\n            res_mask = self.mask_buffer[:bound]\n            res_volume = self.volume_buffer[:bound]\n            if bound < self.mask_buffer.shape[0]:\n                self.mask_buffer = self.mask_buffer[bound:]\n                self.volume_buffer = self.volume_buffer[bound:]\n            else:\n                self.mask_buffer = np.array([])\n                self.volume_buffer = np.array([])\n        else:\n            res_mask = np.array([])\n            res_volume = np.array([])\n        \n        while res_mask.shape[0] < slice_no and idx + self.passes < len(self.paths):\n            patient = self.paths[idx+self.passes]\n            self.passes += 1\n            volumes = []\n            single_mask = patient + mask_path\n            if single_mask == root_train_dir + '/BraTS20_Training_355/BraTS20_Training_355_seg.nii':\n                single_mask = root_train_dir + '/BraTS20_Training_355/W39_1998.09.19_Segm.nii'\n            single_mask = nib.load(single_mask)\n            single_mask = np.asarray(single_mask.dataobj, dtype = np.float)\n            mask = single_mask.transpose(2,0,1)\n            mask_WT = mask.copy()\n            mask_WT[mask_WT == 1] = 0\n            mask_WT[mask_WT == 2] = 1\n            mask_WT[mask_WT == 4] = 0\n\n            mask_TC = mask.copy()\n            mask_TC[mask_TC == 1] = 1\n            mask_TC[mask_TC == 2] = 0\n            mask_TC[mask_TC == 4] = 0\n\n            mask_ET = mask.copy()\n            mask_ET[mask_ET == 1] = 0\n            mask_ET[mask_ET == 2] = 0\n            mask_ET[mask_ET == 4] = 1\n\n            mask_BG = mask.copy()\n            mask_BG[mask_BG == 0] = 3\n            mask_BG[mask_BG == 1] = 0\n            mask_BG[mask_BG == 2] = 0\n            mask_BG[mask_BG == 4] = 0\n            mask_BG[mask_BG == 3] = 1\n\n            mask_full = np.stack([mask_WT, mask_TC, mask_ET, mask_BG])\n            mask_full = np.transpose(mask_full, (1,0,2,3))\n\n            tumor_indices = []\n            mask_pure = []\n            for i in range(155):\n                mask_slice = mask_full[i]\n                if np.sum(mask_slice[0:3]) != 0:\n                    tumor_indices.append(i)\n                    mask_pure.append(mask_slice)\n            mask = np.stack(mask_pure)\n\n            for modality in modalities:\n                single_mod_volume = patient + modality\n                single_mod_volume = nib.load(single_mod_volume)\n                single_mod_volume = np.asarray(single_mod_volume.dataobj, dtype = np.float)\n                single_mod_volume = single_mod_volume.transpose(2,0,1)\n                intermittent = []\n                for i_s in tumor_indices:\n                    intermittent.append(single_mod_volume[i_s])\n                volumes.append(np.stack(intermittent))\n\n            if self.transform:\n                seed = random.randint(0,2**32)\n                random.seed(seed)\n                torch.manual_seed(seed)\n                volumes = np.transpose(np.stack(volumes), (1,0,2,3))\n                volumes = np.stack([self.transform(np.transpose(volumes[i], (1,2,0))) for i in range(np.shape(volumes)[0])])\n                volumes = np.transpose(np.stack(volumes), (1,0,2,3))\n                random.seed(seed)\n                torch.manual_seed(seed)\n                mask = np.stack([self.transform(np.transpose(mask[i], (1,2,0))) for i in range(np.shape(mask)[0])])\n            volumes = [normalize_vol(volumes[i], i) for i in range(np.shape(volumes)[0])]\n            volumes = np.transpose(np.stack(volumes), (1,0,2,3))\n            bound = min(slice_no-res_mask.shape[0], mask.shape[0])\n            if res_mask.size == 0:\n                res_mask = mask[:bound]\n                res_volume = volumes[:bound]\n            else:\n                res_mask = np.concatenate((res_mask, mask[:bound]), axis = 0)\n                res_volume = np.concatenate((res_volume, volumes[:bound]), axis = 0)\n            if bound < mask.shape[0]:\n                if self.mask_buffer.size == 0:\n                    self.mask_buffer = mask[bound:]\n                    self.volume_buffer = volumes[bound:]\n                else:\n                    self.mask_buffer = np.concatenate((self.mask_buffer, mask[bound:]), axis = 0)\n                    self.volume_buffer = np.concatenate((self.volume_buffer, volumes[bound:]), axis = 0)\n        self.passes -= 1\n        res_volume = torch.from_numpy(res_volume).float()\n        res_mask = torch.from_numpy(res_mask).long()\n        return res_volume, res_mask\n    \nclass TestingDataset(Dataset):\n    in_channels = 4\n    out_channels = 4\n    \n    def __init__(self, paths, modalities, mask_path):\n        self.paths = paths\n        self.modalities = modalities\n        self.mask_path = mask_path\n        self.mask_buffer = np.array([])\n        self.volume_buffer = np.array([])\n        \n        \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, idx):\n        patient = self.paths[idx]\n        volumes = []\n        single_mask = patient + mask_path\n        if single_mask == root_train_dir + '/BraTS20_Training_355/BraTS20_Training_355_seg.nii':\n            single_mask = root_train_dir + '/BraTS20_Training_355/W39_1998.09.19_Segm.nii'\n        single_mask = nib.load(single_mask)\n        single_mask = np.asarray(single_mask.dataobj, dtype = np.float)\n        mask = single_mask.transpose(2,0,1)\n        mask_WT = mask.copy()\n        mask_WT[mask_WT == 1] = 0\n        mask_WT[mask_WT == 2] = 1\n        mask_WT[mask_WT == 4] = 0\n\n        mask_TC = mask.copy()\n        mask_TC[mask_TC == 1] = 1\n        mask_TC[mask_TC == 2] = 0\n        mask_TC[mask_TC == 4] = 0\n\n        mask_ET = mask.copy()\n        mask_ET[mask_ET == 1] = 0\n        mask_ET[mask_ET == 2] = 0\n        mask_ET[mask_ET == 4] = 1\n\n        mask_BG = mask.copy()\n        mask_BG[mask_BG == 0] = 3\n        mask_BG[mask_BG == 1] = 0\n        mask_BG[mask_BG == 2] = 0\n        mask_BG[mask_BG == 4] = 0\n        mask_BG[mask_BG == 3] = 1\n        mask_full = np.stack([mask_WT, mask_TC, mask_ET, mask_BG])#, mask_BG\n        mask_full = np.transpose(mask_full, (1,0,2,3))\n        \n        for modality in modalities:\n                single_mod_volume = patient + modality\n                single_mod_volume = nib.load(single_mod_volume)\n                single_mod_volume = np.asarray(single_mod_volume.dataobj, dtype = np.float)\n                single_mod_volume = single_mod_volume.transpose(2,0,1)\n                volumes.append(np.stack(single_mod_volume))\n\n        volumes = [normalize_vol(volumes[i], i) for i in range(np.shape(volumes)[0])]\n        volumes = np.transpose(np.stack(volumes), (1,0,2,3))\n        res_volume = torch.from_numpy(volumes).float()\n        res_mask = torch.from_numpy(mask_full).long()\n\n        return res_volume, res_mask\n\n\n\ntrainset = SegmentationDataset(train_scan_files, modalities, mask_path, 17227, transform = trainTransform)\nvalidset = SegmentationDataset(valid_scan_files, modalities, mask_path, 3661)\ntestset = TestingDataset(test_scan_files, modalities, mask_path)\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=b_s, shuffle=False, num_workers=0)\nvalidloader = torch.utils.data.DataLoader(validset, batch_size=b_s,shuffle=False, num_workers=0)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=b_s, shuffle=False, num_workers=0)","metadata":{"papermill":{"duration":0.134733,"end_time":"2021-06-07T11:01:29.420628","exception":false,"start_time":"2021-06-07T11:01:29.285895","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T20:57:30.347578Z","iopub.execute_input":"2021-06-10T20:57:30.348313Z","iopub.status.idle":"2021-06-10T20:57:30.437715Z","shell.execute_reply.started":"2021-06-10T20:57:30.348254Z","shell.execute_reply":"2021-06-10T20:57:30.436794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neuronska mreža (U-Net)","metadata":{}},{"cell_type":"code","source":"class UnetBlock(nn.Module):\n    def __init__(self, in_channels, features):\n        super(UnetBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, features, kernel_size = 3, padding = 1, bias = False)\n        self.bn1 = nn.BatchNorm2d(features)\n        self.relu = nn.ReLU(inplace = True)\n        self.conv2 = nn.Conv2d(features, features, kernel_size = 3, padding = 1, bias = False)\n        self.bn2 = nn.BatchNorm2d(features)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        return x\n\n    \nclass Unet(nn.Module):\n    def __init__(self, in_channels = 4, single=False, first_features = 32):\n        super(Unet, self).__init__()\n        self.in_channels = in_channels\n        self.first_features = first_features\n        self.single = single\n        \n        self.b1 = UnetBlock(in_channels, first_features)\n        self.pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n        self.b2 = UnetBlock(first_features, first_features*2)\n        self.pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n        self.b3 = UnetBlock(first_features*2, first_features*4)\n        self.pool3 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n        self.b4 = UnetBlock(first_features*4, first_features*8)\n        self.pool4 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n        \n        self.intermediate = UnetBlock(first_features*8, first_features*16)\n        \n        self.uc4 = nn.ConvTranspose2d(first_features*16, first_features*8, kernel_size = 2, stride = 2)\n        self.upb4 = UnetBlock(first_features*16, first_features*8)\n        self.uc3 = nn.ConvTranspose2d(first_features*8, first_features*4, kernel_size = 2, stride = 2)\n        self.upb3 = UnetBlock(first_features*8, first_features*4)\n        self.uc2 = nn.ConvTranspose2d(first_features*4, first_features*2, kernel_size = 2, stride = 2)\n        self.upb2 = UnetBlock(first_features*4, first_features*2)\n        self.uc1 = nn.ConvTranspose2d(first_features*2, first_features, kernel_size = 2, stride = 2)\n        self.upb1 = UnetBlock(first_features*2, first_features)\n        \n        if single:\n            final_features = 1\n        else:\n            final_features = 4\n        self.blend = nn.Conv2d(first_features, final_features, kernel_size=1)\n        \n    def custom(self, module):\n        def custom_forward(*inputs):\n            inputs = module(inputs[0])\n            return inputs\n        return custom_forward\n    \n    def forward(self, x):\n        down1 = cp.checkpoint(self.custom(self.b1), x)\n        down2 = cp.checkpoint(self.custom(self.b2), self.pool1(down1))\n        down3 = cp.checkpoint(self.custom(self.b3), self.pool2(down2))\n        down4 = cp.checkpoint(self.custom(self.b4), self.pool3(down3))\n        \n        intermediate = self.intermediate(self.pool4(down4))\n        \n        \n        up4 = self.uc4(intermediate)\n        up4 = torch.cat((up4, down4), dim=1)\n        up4 = cp.checkpoint(self.custom(self.upb4), up4)\n        up3 = self.uc3(up4)\n        up3 = torch.cat((up3, down3), dim=1)\n        up3 = cp.checkpoint(self.custom(self.upb3), up3)\n        up2 = self.uc2(up3)\n        up2 = torch.cat((up2, down2), dim=1)\n        up2 = cp.checkpoint(self.custom(self.upb2), up2)\n        up1 = self.uc1(up2)\n        up1 = torch.cat((up1, down1), dim=1)\n        up1 = cp.checkpoint(self.custom(self.upb1), up1)\n        \n        logits = self.blend(up1)\n        \n        if self.single:\n            nonlin = nn.Sigmoid()\n        else:\n            nonlin = nn.LogSoftmax(1)\n\n        return nonlin(logits)","metadata":{"papermill":{"duration":0.032371,"end_time":"2021-06-07T11:01:29.462294","exception":false,"start_time":"2021-06-07T11:01:29.429923","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T20:57:30.441536Z","iopub.execute_input":"2021-06-10T20:57:30.441893Z","iopub.status.idle":"2021-06-10T20:57:30.467715Z","shell.execute_reply.started":"2021-06-10T20:57:30.441861Z","shell.execute_reply":"2021-06-10T20:57:30.466731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pomoćne funkcije","metadata":{}},{"cell_type":"code","source":"def dice_score(prediction, ground_truth, smooth = 1.0):\n    prediction = prediction.contiguous().view(-1)\n    ground_truth = ground_truth.contiguous().view(-1)\n    \n    intersection = (prediction*ground_truth).sum()\n    score = (2*intersection+smooth)/(prediction.sum()+ground_truth.sum()+smooth)\n    return score\n\ndef un_one_hot(targets):\n    return targets.argmax(1)\n\ndef output_metrics(loss_avg, dice_scores, phase):\n    print(phase)\n    print(\"Average loss: {}\".format(loss_avg))\n    print(\"Dice score: {}\".format(dice_scores))\n\ndef dataset_evaluate(model, loader, loss_fn, threshold = -1, single = False):\n    global device\n    \n    loss_avg = 0.0\n    dice_scores = []\n    \n    with torch.no_grad():\n        model.eval()\n        for images, ground_truths in loader:\n            images = torch.cat([images[i] for i in range(b_s)])\n            ground_truths = torch.cat([ground_truths[i] for i in range(b_s)])\n            images = images.to(device)\n            ground_truths = ground_truths.to(device)\n            outputs = model(images)\n            loss = loss_fn(outputs, un_one_hot(ground_truths))\n            loss_avg += loss.item()\n            outputs = outputs.argmax(1)\n            ground_truths = ground_truths.argmax(1)\n            outputs_WT = outputs.clone()\n            outputs_WT[outputs_WT == 0] = 1\n            outputs_WT[outputs_WT == 1] = 1\n            outputs_WT[outputs_WT == 2] = 1\n            outputs_WT[outputs_WT == 3] = 0\n            outputs_TC = outputs.clone()\n            outputs_TC[outputs_TC == 0] = 0\n            outputs_TC[outputs_TC == 1] = 1\n            outputs_TC[outputs_TC == 2] = 1\n            outputs_TC[outputs_TC == 3] = 0\n            outputs_ET = outputs.clone()\n            outputs_ET[outputs_ET == 0] = 0\n            outputs_ET[outputs_ET == 1] = 0\n            outputs_ET[outputs_ET == 2] = 1\n            outputs_ET[outputs_ET == 3] = 0\n            ground_truths_WT = ground_truths.clone()\n            ground_truths_WT[ground_truths_WT == 0] = 1\n            ground_truths_WT[ground_truths_WT == 1] = 1\n            ground_truths_WT[ground_truths_WT == 2] = 1\n            ground_truths_WT[ground_truths_WT == 3] = 0\n            ground_truths_TC = ground_truths.clone()\n            ground_truths_TC[ground_truths_TC == 0] = 0\n            ground_truths_TC[ground_truths_TC == 1] = 1\n            ground_truths_TC[ground_truths_TC == 2] = 1\n            ground_truths_TC[ground_truths_TC == 3] = 0\n            ground_truths_ET = ground_truths.clone()\n            ground_truths_ET[ground_truths_ET == 0] = 0\n            ground_truths_ET[ground_truths_ET == 1] = 0\n            ground_truths_ET[ground_truths_ET == 2] = 1\n            ground_truths_ET[ground_truths_ET == 3] = 0\n            \n            dice_scores.append([dice_score(outputs_WT, ground_truths_WT),\n                                dice_score(outputs_TC, ground_truths_TC),\n                                dice_score(outputs_ET, ground_truths_ET)])\n                \n    loss_avg /= len(loader)\n    model.train()\n    dice_scores = np.stack(dice_scores)\n    return loss_avg, dice_scores.mean(0)","metadata":{"papermill":{"duration":0.031344,"end_time":"2021-06-07T11:01:29.537072","exception":false,"start_time":"2021-06-07T11:01:29.505728","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T20:57:30.469641Z","iopub.execute_input":"2021-06-10T20:57:30.470298Z","iopub.status.idle":"2021-06-10T20:57:30.494421Z","shell.execute_reply.started":"2021-06-10T20:57:30.47025Z","shell.execute_reply":"2021-06-10T20:57:30.492941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Iscrtavanje grafa gubitka","metadata":{}},{"cell_type":"code","source":"def plot_progress(data):\n    valid_loss = data['valid loss']\n    train_loss = data['train loss']\n\n    fig, ax = plt.subplots(figsize=(16,8))\n    linewidth = 2\n    legend_size = 10\n    train_color = 'm'\n    val_color = 'c'\n\n    ax.set_title('Loss')\n    ax.plot(train_loss, marker='o', color=train_color,\n           linewidth=linewidth, linestyle='-', label='train')\n    ax.plot(valid_loss, marker='o', color=val_color,\n           linewidth=linewidth, linestyle='-', label='validation')\n    ax.legend(loc='upper right', fontsize=legend_size)\n\n    save_path = os.path.join('./', 'loss.png')\n    print('Plotting in: ', save_path)\n    plt.savefig(save_path)\n    return","metadata":{"papermill":{"duration":0.024687,"end_time":"2021-06-07T11:01:29.570602","exception":false,"start_time":"2021-06-07T11:01:29.545915","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T20:57:30.495704Z","iopub.execute_input":"2021-06-10T20:57:30.498286Z","iopub.status.idle":"2021-06-10T20:57:30.510071Z","shell.execute_reply.started":"2021-06-10T20:57:30.498256Z","shell.execute_reply":"2021-06-10T20:57:30.509067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Treniranje i evaluacija modela","metadata":{}},{"cell_type":"code","source":"def train_network():\n    global device\n    plot_data = {}\n    plot_data[\"train loss\"] = []\n    plot_data[\"valid loss\"] = []\n    plot_data[\"WT valid dice\"] = []\n    plot_data[\"TC valid dice\"] = []\n    plot_data[\"ET valid dice\"] = []\n    plot_data[\"test loss\"] = 0\n    plot_data[\"WT test dice\"] = []\n    plot_data[\"TC test dice\"] = []\n    plot_data[\"ET test dice\"] = []\n    plot_data[\"lr\"] = []\n    \n    SAVE_PATH = \"./network.pt\"\n    print(\"device:\", device)\n    \n    net = Unet().float()\n    net = net.to(device)\n    lossFunc = nn.NLLLoss(weight = torch.tensor([2,5,3,1]).float().to(device))\n    \n    optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay = weight_decay)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = num_epochs, eta_min = 1e-6)\n    \n    for e in range(num_epochs):\n        accLoss = 0.\n        net.train()\n        i = -1\n        for inputs, ground_truths in trainloader:\n            i+=1\n            inputs = torch.cat([inputs[i] for i in range(b_s)])\n            ground_truths = torch.cat([ground_truths[i] for i in range(b_s)])\n            inputs.requires_grad = True\n            inputs = inputs.to(device)\n            ground_truths = ground_truths.to(device)\n            \n            outputs = net(inputs)\n            loss = lossFunc(outputs, un_one_hot(ground_truths))\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            accLoss += loss.item()\n\n            if i % 10 == 0:\n                print(\"Epoch: %d, Iteration: %5d, Loss: %.3f\" % ((e + 1), (i+1), (accLoss / (i + 1))))\n        trainset.reset()\n\n        val_loss, val_dice = dataset_evaluate(net, validloader, lossFunc)\n        validset.reset()\n        output_metrics(val_loss, val_dice, \"Validation\")\n        plot_data[\"valid loss\"].append(val_loss)\n        plot_data[\"WT valid dice\"].append(val_dice[0])\n        plot_data[\"TC valid dice\"].append(val_dice[1])\n        plot_data[\"ET valid dice\"].append(val_dice[2])\n        \n        plot_data[\"lr\"].append(scheduler.get_last_lr())\n        plot_data[\"train loss\"].append(accLoss/(i+1))\n        \n        scheduler.step()\n    \n    test_loss, test_dice = dataset_evaluate(net, testloader, lossFunc)\n    output_metrics(test_loss, test_dice, \"Test:\")\n    \n    plot_data[\"test loss\"] = test_loss\n    plot_data[\"WT test dice\"] = test_dice[0]\n    plot_data[\"TC test dice\"] = test_dice[1]\n    plot_data[\"ET test dice\"] = test_dice[2]\n    \n    torch.save(net.state_dict(), SAVE_PATH)\n    \n    with open(\"./epoch_data.txt\", 'w') as f:\n        f.write(repr(plot_data))\n\n    plot_progress(plot_data)\n\n    return plot_data\n    \n    \nepoch_data = train_network()","metadata":{"papermill":{"duration":31043.575328,"end_time":"2021-06-07T19:38:53.154888","exception":false,"start_time":"2021-06-07T11:01:29.57956","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}
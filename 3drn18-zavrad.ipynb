{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lovrorabuzin/3drn18-zavrad?scriptVersionId=91692024\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Importi i hiperparametri","metadata":{"papermill":{"duration":0.016977,"end_time":"2021-05-02T19:27:21.069118","exception":false,"start_time":"2021-05-02T19:27:21.052141","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install inplace-abn","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:37:06.484453Z","iopub.execute_input":"2021-06-10T22:37:06.484836Z","iopub.status.idle":"2021-06-10T22:37:13.795541Z","shell.execute_reply.started":"2021-06-10T22:37:06.484804Z","shell.execute_reply":"2021-06-10T22:37:13.794333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import inplace_abn as iabn\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional\nimport torch.nn.functional as F\n\nimport gc\n\nimport sklearn\nfrom sklearn import model_selection\n\nimport torchvision\nimport torchvision.transforms as torch_transforms\nimport torch.utils.data as data\nimport torch.optim as optim\n\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import Subset\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\n\nimport pickle\nimport numpy as np\nimport math\nimport pandas as pd\n\nimport skimage as ski\nimport skimage.io\nimport os\n\nimport nibabel as nib\nfrom PIL import Image\nimport imageio\n\nimport sys\nimport json\n\nb_s = 7\nlearning_rate = 0.001\nweight_decay = 0.005\ngamma = 0.95\nnum_epochs = 10\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nroot_train_dir = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":2.826937,"end_time":"2021-05-02T19:27:33.801702","exception":false,"start_time":"2021-05-02T19:27:30.974765","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T22:37:13.800586Z","iopub.execute_input":"2021-06-10T22:37:13.800968Z","iopub.status.idle":"2021-06-10T22:37:13.813047Z","shell.execute_reply.started":"2021-06-10T22:37:13.800937Z","shell.execute_reply":"2021-06-10T22:37:13.811671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Učitavanje podataka","metadata":{"papermill":{"duration":0.021088,"end_time":"2021-05-02T19:27:41.681844","exception":false,"start_time":"2021-05-02T19:27:41.660756","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#Ovaj dio koda preuzet je s https://github.com/jcreinhold/niftidataset jer sam imao problema s instalacijom paketa\n\nfrom typing import Optional, Tuple, Union\n\nclass BaseTransform:\n    def __repr__(self): return f'{self.__class__.__name__}'\n\ndef normalize3d(tensor, mean, std, inplace=False):\n    \"\"\"\n    normalize a 3d tensor\n    Args:\n        tensor (Tensor): Tensor image of size (C, H, W, D) to be normalized.\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n    Returns:\n        Tensor: Normalized Tensor image.\n    \"\"\"\n    if not inplace:\n        tensor = tensor.clone()\n\n    mean = torch.as_tensor(mean, dtype=torch.float32, device=tensor.device)\n    std = torch.as_tensor(std, dtype=torch.float32, device=tensor.device)\n    tensor.sub_(mean[:, None, None, None]).div_(std[:, None, None, None])\n    return tensor\n\nclass Normalize:\n    \"\"\"\n    Implement a normalize function for input two images.\n    It computes std and mean for each input Tensor if mean and std equal to None,\n     then the function normalizes Tensor using the computed values.\n    Args:\n        mean: mean of input Tensor. if None passed, mean of each Tensor will be computed and normalization will be performed based on computed mean.\n        std: standard deviation of input Tensor. if None passed, std of each Tensor will be computed and normalization will be performed based on computed std.\n        tfm_x (bool): transform x or not\n        tfm_y (bool): transform y or not\n        is_3d (bool): is the Tensor 3d or not. this causes to normalize the Tensor on each channel.\n    \"\"\"\n\n    def __init__(self, mean=None, std=None, tfm_x: bool = True, tfm_y: bool = False,\n                 is_3d: bool = False):\n        self.mean = mean\n        self.std = std\n        self.tfm_x = tfm_x\n        self.tfm_y = tfm_y\n        self.is_3d = is_3d\n\n    def _tfm(self, tensor: torch.Tensor):\n        if self.is_3d:\n            norm = normalize3d\n            mean = torch.as_tensor(self.mean, dtype=torch.float32, device=tensor.device) if not (\n                    self.mean is None) else tensor.mean(dim=(1, 2, 3))\n            std = torch.as_tensor(self.std, dtype=torch.float32, device=tensor.device) if not (\n                    self.std is None) else tensor.std(dim=(1, 2, 3))\n            # to prevent division by zero\n            std[std == 0.] = 1e-6\n        else:\n            norm = torchvision.transforms.functional.normalize\n            mean = self.mean if not (self.mean is None) else tensor.mean().item()\n            std = self.std if not (self.std is None) else tensor.std().item()\n            # to prevent division by zero\n            if std == 0.:\n                std = 1e-6\n        return norm(tensor, mean, std)\n\n    def __call__(self, sample: Tuple[torch.Tensor, torch.Tensor]):\n        src, tgt = sample\n        if self.tfm_x: src = self._tfm(src)\n        if self.tfm_y: tgt = self._tfm(tgt)\n        return src, tgt\n\n    def __repr__(self):\n        s = '{name}(mean={mean}, std={std}, tfm_x={tfm_x}, tfm_y={tfm_y}, is_3d={is_3d})'\n        d = dict(self.__dict__)\n        return s.format(name=self.__class__.__name__, **d)\n\nclass ToTensor(BaseTransform):\n    \"\"\" Convert images in sample to Tensors \"\"\"\n\n    def __init__(self, color=False):\n        self.color = color\n\n    def __call__(self, sample: Tuple[np.ndarray, np.ndarray]) -> Tuple[torch.Tensor, torch.Tensor]:\n        src, tgt = sample\n        if isinstance(src, np.ndarray) and isinstance(tgt, np.ndarray):\n            return torch.from_numpy(src), torch.from_numpy(tgt)\n        if isinstance(src, list): src = np.stack(src)\n        if isinstance(tgt, list): src = np.stack(tgt)\n        # handle PIL images\n        src, tgt = np.asarray(src), np.asarray(tgt)\n        if src.ndim == 3 and self.color: src = src.transpose((2, 0, 1)).astype(np.float32)\n        if tgt.ndim == 3 and self.color: tgt = tgt.transpose((2, 0, 1)).astype(np.float32)\n        if src.ndim == 2: src = src[None, ...]  # add channel dimension\n        if tgt.ndim == 2: tgt = tgt[None, ...]\n        return torch.from_numpy(src.copy()), torch.from_numpy(tgt.copy())\n\n__all__ = ['NiftiDataset',\n           'MultimodalNiftiDataset',\n           'MultimodalNifti2p5DDataset',\n           'MultimodalImageDataset',\n           'train_val_split']\n\nfrom typing import Callable, List, Optional\n\nimport nibabel as nib\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data.dataset import Dataset\n\nfrom typing import List, Tuple\n\nfrom glob import glob\nimport os\n\n\ndef split_filename(filepath: str) -> Tuple[str, str, str]:\n    \"\"\" split a filepath into the directory, base, and extension \"\"\"\n    path = os.path.dirname(filepath)\n    filename = os.path.basename(filepath)\n    base, ext = os.path.splitext(filename)\n    if ext == '.gz':\n        base, ext2 = os.path.splitext(base)\n        ext = ext2 + ext\n    return path, base, ext\n\n\ndef glob_imgs(path: str, ext='*.nii*') -> List[str]:\n    \"\"\" grab all `ext` files in a directory and sort them for consistency \"\"\"\n    fns = sorted(glob(os.path.join(path, ext)))\n    return fns\n\n\nclass NiftiDataset(Dataset):\n    \"\"\"\n    create a dataset class in PyTorch for reading NIfTI files\n\n    Args:\n        source_fns (List[str]): list of paths to source images\n        target_fns (List[str]): list of paths to target images\n        transform (Callable): transform to apply to both source and target images\n        preload (bool): load all data when initializing the dataset\n    \"\"\"\n\n    def __init__(self, source_fns: List[str], target_fns: List[str], transform: Optional[Callable] = None,\n                 preload: bool = False):\n        self.source_fns, self.target_fns = source_fns, target_fns\n        self.transform = transform\n        self.preload = preload\n        if len(self.source_fns) != len(self.target_fns) or len(self.source_fns) == 0:\n            raise ValueError(f'Number of source and target images must be equal and non-zero')\n        if preload:\n            self.imgs = [(nib.load(s).get_data(), nib.load(t).get_data())\n                         for s, t in zip(self.source_fns, self.target_fns)]\n\n    @classmethod\n    def setup_from_dir(cls, source_dir: str, target_dir: str, transform: Optional[Callable] = None,\n                       preload: bool = False):\n        source_fns, target_fns = glob_imgs(source_dir), glob_imgs(target_dir)\n        return cls(source_fns, target_fns, transform, preload)\n\n    def __len__(self):\n        return len(self.source_fns)\n\n    def __getitem__(self, idx: int):\n        if not self.preload:\n            src_fn, tgt_fn = self.source_fns[idx], self.target_fns[idx]\n            sample = (nib.load(src_fn).get_fdata(dtype=np.float32), nib.load(tgt_fn).get_fdata(dtype=np.float32))\n        else:\n            sample = self.imgs[idx]\n        if self.transform is not None:\n            sample = self.transform(sample)\n        return sample\n\n\nclass MultimodalDataset(Dataset):\n    \"\"\" base class for Multimodal*Dataset \"\"\"\n\n    def __init__(self, source_fns: List[List[str]], target_fns: List[List[str]],\n                 transform: Optional[Callable] = None,\n                 segmentation: bool = False, preload: bool = False, **kwargs):\n        self.source_fns, self.target_fns = source_fns, target_fns\n        self.transform = transform\n        self.segmentation = segmentation\n        self.preload = preload\n        if any([len(self.source_fns[0]) != len(sfn) for sfn in self.source_fns]) or \\\n                any([len(self.target_fns[0]) != len(tfn) for tfn in self.target_fns]) or \\\n                len(self.source_fns[0]) != len(self.target_fns[0]) or \\\n                len(self.source_fns[0]) == 0:\n            raise ValueError(f'Number of source and target images must be equal and non-zero')\n        if preload:\n            self.imgs = []\n            for idx in range(len(self.source_fns[0])):\n                src_fns, tgt_fns = [sfns[idx] for sfns in self.source_fns], [tfns[idx] for tfns in self.target_fns]\n                self.imgs.append((self.stack([self.get_data(s) for s in src_fns]),\n                                  self.stack([self.get_data(t) for t in tgt_fns])))\n\n    @classmethod\n    def setup_from_dir(cls, source_dirs: List[str], target_dirs: List[str],\n                       transform: Optional[Callable] = None, segmentation: bool = False,\n                       preload: bool = False, ext: str = '*.nii*', **kwargs):\n        source_fns = [glob_imgs(sd, ext) for sd in source_dirs]\n        target_fns = [glob_imgs(td, ext) for td in target_dirs]\n        return cls(source_fns, target_fns, transform, segmentation, preload, **kwargs)\n\n    def __len__(self):\n        return len(self.source_fns[0])\n\n    def __getitem__(self, idx: int):\n        if not self.preload:\n            src_fns, tgt_fns = [sfns[idx] for sfns in self.source_fns], [tfns[idx] for tfns in self.target_fns]\n            sample = (self.stack([self.get_data(s) for s in src_fns]),\n                      self.stack([self.get_data(t) for t in tgt_fns]))\n        else:\n            sample = self.imgs[idx]\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.segmentation:\n            sample = (sample[0], sample[1].long()[0])  # for segmentation, loss expects no channel dim\n        return sample\n\n    def get_data(self, fn):\n        raise NotImplementedError\n\n    def stack(self, imgs):\n        raise NotImplementedError\n\n\nclass MultimodalNiftiDataset(MultimodalDataset):\n    \"\"\"\n    create a dataset class in PyTorch for reading N types of NIfTI files to M types of output NIfTI files\n\n    ** note that all images must have the same dimensions! **\n\n    Args:\n        source_dirs (List[str]): paths to source images\n        target_dirs (List[str]): paths to target images\n        transform (Callable): transform to apply to both source and target images\n    \"\"\"\n\n    def get_data(self, fn): return nib.load(fn).get_fdata(dtype=np.float32)\n\n    def stack(self, imgs): return np.stack(imgs)\n\n\nclass MultimodalNifti2p5DDataset(MultimodalNiftiDataset):\n    \"\"\"\n    create a dataset class in PyTorch for reading N types of NIfTI files to M types of output NIfTI files\n    2.5D dataset, so return images stacked in the channel dimension for processing with a 2D CNN\n\n    ** note that all images must have the same dimensions! **\n\n    Args:\n        source_dirs (List[str]): paths to source images\n        target_dirs (List[str]): paths to target images\n        transform (Callable): transform to apply to both source and target images\n    \"\"\"\n\n    def __init__(self, source_dirs: List[str], target_dirs: List[str], transform: Optional[Callable] = None,\n                 segmentation: bool = False, preload: bool = False, axis: int = 0):\n        self.axis = axis\n        super().__init__(source_dirs, target_dirs, transform, segmentation, preload)\n\n    def stack(self, imgs): return np.swapaxes(np.concatenate(imgs, axis=self.axis), 0, self.axis)\n\n\nclass MultimodalImageDataset(MultimodalDataset):\n    \"\"\"\n    create a dataset class in PyTorch for reading N types of (no channel) image files to M types of output image files.\n    can use whatever PIL can open.\n\n    ** note that all images must have the same dimensions! **\n\n    There is no implementation of ImageDataset because it is sufficient to use normal pytorch image\n    dataloaders for that use case\n\n    Args:\n        source_dirs (List[str]): paths to source images\n        target_dirs (List[str]): paths to target images\n        transform (Callable): transform to apply to both source and target images\n        color (bool): images are color, ie, 3 channels\n    \"\"\"\n\n    def __init__(self, source_dirs: List[str], target_dirs: List[str], transform: Optional[Callable] = None,\n                 segmentation: bool = False, color: bool = False, preload: bool = False):\n        self.color = color\n        super().__init__(source_dirs, target_dirs, transform, segmentation, preload)\n\n    @classmethod\n    def setup_from_dir(cls, source_dirs: List[str], target_dirs: List[str],\n                       transform: Optional[Callable] = None,\n                       segmentation: bool = False,\n                       color: bool = False, preload: bool = False,\n                       ext: str = '*.tif*', **kwargs):\n        source_fns = [glob_imgs(sd, ext) for sd in source_dirs]\n        target_fns = [glob_imgs(td, ext) for td in target_dirs]\n        return cls(source_fns, target_fns, transform, segmentation, color, preload)\n\n    def get_data(self, fn):\n        data = np.asarray(Image.open(fn), dtype=np.float32)\n        if self.color: data = data.transpose((2, 0, 1))\n        return data\n\n    def stack(self, imgs):\n        data = np.stack(imgs)\n        if self.color: data = data.squeeze()\n        return data\n\n\ndef train_val_split(source_dir: str, target_dir: str, valid_pct: float = 0.2,\n                    transform: Optional[Callable] = None, preload: bool = False):\n    \"\"\"\n    create two separate NiftiDatasets in PyTorch for working with NifTi files. If a directory contains source files\n    and the other one contains target files and also you dont have a specific directory for validation set,\n    this function splits data to two NiftiDatasets randomly with given percentage.\n\n    Args:\n        source_dir (str): path to source images.\n        target_dir (str): path to target images.\n        valid_pct (float): percent of validation set from data.\n        transform (Callable): transform to apply to both source and target images.\n        preload: load all data when initializing the dataset\n    Returns:\n        Tuple: (train_dataset, validation_dataset).\n    \"\"\"\n    if not (0 < valid_pct < 1):\n        raise ValueError(f'valid_pct must be between 0 and 1')\n    source_fns, target_fns = glob_imgs(source_dir), glob_imgs(target_dir)\n    rand_idx = np.random.permutation(list(range(len(source_fns))))\n    cut = int(valid_pct * len(source_fns))\n    return (NiftiDataset(source_fns=[source_fns[i] for i in rand_idx[cut:]],\n                         target_fns=[target_fns[i] for i in rand_idx[cut:]],\n                         transform=transform, preload=preload),\n            NiftiDataset(source_fns=[source_fns[i] for i in rand_idx[:cut]],\n                         target_fns=[target_fns[i] for i in rand_idx[:cut]],\n                         transform=transform, preload=preload))\n","metadata":{"papermill":{"duration":0.044522,"end_time":"2021-05-02T19:27:41.747797","exception":false,"start_time":"2021-05-02T19:27:41.703275","status":"completed"},"tags":[],"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-10T22:37:13.814996Z","iopub.execute_input":"2021-06-10T22:37:13.815542Z","iopub.status.idle":"2021-06-10T22:37:13.890829Z","shell.execute_reply.started":"2021-06-10T22:37:13.815469Z","shell.execute_reply":"2021-06-10T22:37:13.88939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tensorize = ToTensor()\nnormalize = Normalize(is_3d = True)\nreorder = torch_transforms.Lambda(lambda t: (t[0].permute(0,3,1,2), t[1].permute(0,3,1,2)))\n\ntr = torch_transforms.Compose([tensorize, normalize, reorder])\n\nname_mapping = pd.read_csv(root_train_dir + '/name_mapping.csv')\n\nname_mapping.rename({'BraTS_2020_subject_ID': 'ID'}, axis=1, inplace=True)\n\nsurvival_info = pd.read_csv(root_train_dir + '/survival_info.csv')\n\nsurvival_info.rename({'Brats20ID': 'ID'}, axis=1, inplace=True)\n\npatient_info = survival_info.merge(name_mapping, on=\"ID\", how=\"right\")\n\nmodalities = ['_flair.nii', '_t1.nii', '_t1ce.nii', '_t2.nii']\n\ntrain_images = []\nvalid_images = []\ntest_images = []\n\nHGG_names = list(patient_info[patient_info['Grade'] == 'HGG'].ID)\nLGG_names = list(patient_info[patient_info['Grade'] == 'LGG'].ID)\n\nfor i in range(len(HGG_names)):\n    image_path = root_train_dir+'/'+HGG_names[i]+'/'+HGG_names[i]\n    if i % 7 == 5:\n        valid_images.append(image_path)\n    elif i % 7 == 4:\n        test_images.append(image_path)\n    else:\n        train_images.append(image_path)\n\nfor i in range(len(LGG_names)):\n    image_path = root_train_dir+'/'+LGG_names[i]+'/'+LGG_names[i]\n    if i % 7 == 5:\n        valid_images.append(image_path)\n    elif i % 7 == 4:\n        test_images.append(image_path)\n    else:\n        train_images.append(image_path)\n\ntrain_sources = []\nvalid_sources = []\ntest_sources = []\n\nmodality_names=[]\n\nfor modality in modalities:\n    modality_names.append(modality[1:-4])\n    train_sources.append([image + modality for image in train_images])\n    valid_sources.append([image + modality for image in valid_images])\n    test_sources.append([image + modality for image in test_images])\n\ntrain_targets = []\nvalid_targets = []\ntest_targets = []\n\nfor modality in modalities:\n    train_targets.append([image + '_seg.nii' for image in train_images])\n    valid_targets.append([image + '_seg.nii' for image in valid_images])\n    test_targets.append([image + '_seg.nii' for image in test_images])\n    \n    real_name = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_355/W39_1998.09.19_Segm.nii'\n    fake_name = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_355/BraTS20_Training_355_seg.nii'\n    valid_targets = [[real_name if (x == fake_name) else x for x in valid_targets_in] for valid_targets_in in valid_targets]\n\ntrainset = MultimodalNiftiDataset(train_sources, train_targets, tr, segmentation = True)\nvalidset = MultimodalNiftiDataset(valid_sources, valid_targets, tr, segmentation = True)\ntestset = MultimodalNiftiDataset(test_sources, test_targets, tr, segmentation = True)\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=b_s, shuffle=True, num_workers=0)\nvalidloader = torch.utils.data.DataLoader(validset, batch_size=b_s,shuffle=True, num_workers=0)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=b_s, shuffle=False, num_workers=0)","metadata":{"papermill":{"duration":0.916884,"end_time":"2021-05-02T19:27:42.773144","exception":false,"start_time":"2021-05-02T19:27:41.85626","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T22:37:13.893274Z","iopub.execute_input":"2021-06-10T22:37:13.893831Z","iopub.status.idle":"2021-06-10T22:37:13.941865Z","shell.execute_reply.started":"2021-06-10T22:37:13.893782Z","shell.execute_reply":"2021-06-10T22:37:13.940971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neuronska mreža (3D ResNet18+SPP)","metadata":{"papermill":{"duration":0.022209,"end_time":"2021-05-02T19:27:42.984186","exception":false,"start_time":"2021-05-02T19:27:42.961977","status":"completed"},"tags":[]}},{"cell_type":"code","source":"upsample = lambda x, size: F.interpolate(x, size, mode='trilinear', align_corners=False)\n\nclass ResidualBlock(nn.Module):\n    expansion = 1\n    def __init__(self, in_channels, channels, stride=1, identity_downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.bn1 = iabn.ABN(in_channels)\n        self.conv1 = nn.Conv3d(in_channels, channels, kernel_size = 3, stride = stride, padding = 1, bias = False)\n        self.bn2 = iabn.ABN(channels)\n        self.conv2 = nn.Conv3d(channels, channels, kernel_size = 3, padding = 1, bias = False)\n        self.identity_downsample = identity_downsample\n        self.stride = stride\n\n    def forward(self, x):        \n        x = self.bn1(x)\n        residual = x.clone()\n        x = self.conv1(x)\n        x = self.bn2(x)\n        x = self.conv2(x)\n        if self.identity_downsample is not None:\n            residual = self.identity_downsample(residual)\n        x += residual\n        return x, x\n\nclass BNReLUConv(nn.Sequential):\n    def __init__(self, in_channels, out_channels, k=3):\n        super(BNReLUConv, self).__init__()\n        self.add_module('BN', iabn.ABN(in_channels))\n        padding = k // 2\n        self.add_module('Conv', nn.Conv3d(in_channels, out_channels, k, padding=padding, bias=False))\n        \nclass Up(nn.Module):\n    def __init__(self, in_channels, skip_channels, out_channels, k=3):\n        super(Up, self).__init__()\n        self.bottleneck = BNReLUConv(skip_channels, in_channels, k = 1)\n        self.blend = BNReLUConv(in_channels, out_channels, k = k)\n        self.upsample = lambda x, size: F.interpolate(x, size, mode='trilinear', align_corners=False)\n    \n    def forward(self, x, skip):\n        skip = self.bottleneck.forward(skip)\n        skip_size = skip.size()[2:5]\n        x = self.upsample(x, skip_size)\n        x = x + skip\n        x = self.blend.forward(x)\n        return x\n\nclass SPP(nn.Module):\n    def __init__(self, in_channels, num_levels, bt_size, level_size, out_size, grids):\n        super(SPP, self).__init__()\n        self.upsample = lambda x, size: F.interpolate(x, size, mode='trilinear', align_corners=False)\n        self.spp = nn.Sequential()\n        self.spp.add_module('spp_bn', BNReLUConv(in_channels, bt_size, k=1))\n        self.grids = grids\n        num_features = bt_size\n        final_size = num_features\n        for i in range(num_levels):\n            final_size += level_size\n            self.spp.add_module('spp' + str(i), BNReLUConv(num_features, level_size, k=1))\n        self.spp.add_module('spp_fuse', BNReLUConv(final_size, out_size, k=1))\n        \n    def forward(self, x):\n        levels = []\n        target_size = x.size()[2:5]\n        ar = target_size[0]/target_size[1]\n        x = self.spp[0].forward(x)\n        levels.append(x)\n        num = len(self.spp) - 1\n        for i in range(1, num):\n            grid_size = (max(1, round(ar * self.grids[i - 1])), self.grids[i-1], self.grids[i-1]) # \n            x_pooled = F.adaptive_avg_pool3d(x, grid_size) #\n            level = self.spp[i].forward(x_pooled)\n            level = self.upsample(level, target_size)\n            levels.append(level)\n        x = torch.cat(levels, 1)\n        x = self.spp[-1].forward(x)\n        return x\n\nclass ResNet(nn.Module):\n    def __init__(self, residual_block, layers, image_channels=4):\n        super(ResNet, self).__init__()\n        self.inplanes = 32\n        self.num_features = 128\n        self.bn1 = iabn.ABN(4)\n        self.conv1 = nn.Conv3d(4, 32, kernel_size = 7, stride = 2, padding = 3, bias = False)\n        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(residual_block, 32, layers[0])\n        self.layer2 = self._make_layer(residual_block, 64, layers[1], stride=2)\n        self.layer3 = self._make_layer(residual_block, 128, layers[2], stride=2)\n        self.layer4 = self._make_layer(residual_block, 256, layers[3], stride=2)\n        num_levels = 3\n        self.spp_size = 128\n        bt_size = self.spp_size\n        level_size = self.spp_size // num_levels\n        self.spp = SPP(self.inplanes, num_levels, bt_size=bt_size, level_size=level_size, out_size=128, grids = (4,2,1))\n        self.upblend = nn.Conv3d(128, 4, kernel_size = 3, stride = 1, padding = 1)\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n    \n    def forward_resblock(self, x, layers):\n        skip = None\n        for l in layers:\n            x = l(x)\n            if isinstance(x, tuple):\n                x, skip = x\n        return x, skip\n\n    def forward_down(self, image):\n        x = self.bn1(image)\n        x = self.conv1(x)\n        x = self.maxpool(x)\n        features = []\n        x, skip = self.forward_resblock(x, self.layer1)\n        x, skip = self.forward_resblock(x, self.layer2)\n        x, skip = self.forward_resblock(x, self.layer3)\n        x, skip = self.forward_resblock(x, self.layer4)\n        x = self.spp.forward(skip)\n        image = self.upblend(x)\n        return image\n\n    def forward(self, image):\n        return self.forward_down(image)\n    \n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            layers = [iabn.ABN(self.inplanes)]\n            layers += [nn.Conv3d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False)]\n            downsample = nn.Sequential(*layers)\n        layers = [block(self.inplanes, planes, stride, downsample)]\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers += [block(self.inplanes, planes)]\n        return nn.Sequential(*layers)\n\nclass SemSegModel(nn.Module):\n    def __init__(self, backbone, num_classes):\n        super(SemSegModel, self).__init__()\n        self.backbone = backbone\n        self.num_classes = num_classes\n        self.logits = BNReLUConv(backbone.num_features, self.num_classes)\n        self.multiscale_factors=(.5, .75, 1.5, 2.)\n        \n    def forward(self, image):\n        image_size = image.size()[2:5]\n        features = self.backbone(image)\n        logits = upsample(features, image_size)\n        return logits","metadata":{"papermill":{"duration":0.066176,"end_time":"2021-05-02T19:27:43.072659","exception":false,"start_time":"2021-05-02T19:27:43.006483","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T22:37:13.945827Z","iopub.execute_input":"2021-06-10T22:37:13.946181Z","iopub.status.idle":"2021-06-10T22:37:13.988585Z","shell.execute_reply.started":"2021-06-10T22:37:13.946132Z","shell.execute_reply":"2021-06-10T22:37:13.987135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pomoćne funkcije","metadata":{}},{"cell_type":"code","source":"def split_segments(target, batch_size):\n    layers = []\n    for i in range(batch_size):\n        gt_bg = torch.zeros(target[i].shape)\n        gt_pe = torch.zeros(target[i].shape)\n        gt_ec = torch.zeros(target[i].shape)\n        gt_nc = torch.zeros(target[i].shape)\n        gt_bg[target[i] == 0] = 1\n        gt_pe[target[i] == 2] = 1\n        gt_ec[target[i] == 4] = 1\n        gt_nc[target[i] == 1] = 1\n        layers.append(torch.stack([gt_pe,gt_nc,gt_ec,gt_bg]))\n    return torch.stack(layers)\n\ndef un_one_hot(target):\n    return(target.argmax(1))\n\ndef dice_score(prediction, ground_truth, smooth = 1.0):\n    n = prediction.shape[0]\n    score_acc = 0\n    for i in range(n):\n        prediction_inner = prediction[i].contiguous().view(-1)\n        ground_truth_inner = ground_truth[i].contiguous().view(-1)\n    \n        intersection = (prediction_inner*ground_truth_inner).sum()\n        score = (2*intersection+smooth)/(prediction_inner.sum()+ground_truth_inner.sum()+smooth)\n        score_acc+=score\n    return score/n\n\ndef evaluate(model, loader, loss_fn):\n    global device\n    \n    loss_avg = 0.0\n    dice_scores = []\n    \n    with torch.no_grad():\n        model.eval()\n        for images, ground_truths in loader:\n            images = images.to(device)\n            ground_truths = ground_truths.to(device)\n            outputs = model(images)\n            loss = loss_fn(outputs, un_one_hot(split_segments(ground_truths, ground_truths.shape[0])).to(device=device))\n            loss_avg += loss.item()\n            outputs = nn.LogSoftmax(1)(outputs)\n            outputs = outputs.argmax(1)\n            ground_truths = un_one_hot(split_segments(ground_truths, ground_truths.shape[0]).to(device=device))\n\n            outputs_WT = outputs.clone()\n            outputs_WT[outputs_WT == 0] = 1\n            outputs_WT[outputs_WT == 1] = 1\n            outputs_WT[outputs_WT == 2] = 1\n            outputs_WT[outputs_WT == 3] = 0\n\n            outputs_TC = outputs.clone()\n            outputs_TC[outputs_TC == 0] = 0\n            outputs_TC[outputs_TC == 1] = 1\n            outputs_TC[outputs_TC == 2] = 1\n            outputs_TC[outputs_TC == 3] = 0\n\n            outputs_ET = outputs.clone()\n            outputs_ET[outputs_ET == 0] = 0\n            outputs_ET[outputs_ET == 1] = 0\n            outputs_ET[outputs_ET == 2] = 1\n            outputs_ET[outputs_ET == 3] = 0\n            \n            ground_truths_WT = ground_truths.clone()\n            ground_truths_WT[ground_truths_WT == 0] = 1\n            ground_truths_WT[ground_truths_WT == 1] = 1\n            ground_truths_WT[ground_truths_WT == 2] = 1\n            ground_truths_WT[ground_truths_WT == 3] = 0\n\n            ground_truths_TC = ground_truths.clone()\n            ground_truths_TC[ground_truths_TC == 0] = 0\n            ground_truths_TC[ground_truths_TC == 1] = 1\n            ground_truths_TC[ground_truths_TC == 2] = 1\n            ground_truths_TC[ground_truths_TC == 3] = 0\n\n            ground_truths_ET = ground_truths.clone()\n            ground_truths_ET[ground_truths_ET == 0] = 0\n            ground_truths_ET[ground_truths_ET == 1] = 0\n            ground_truths_ET[ground_truths_ET == 2] = 1\n            ground_truths_ET[ground_truths_ET == 3] = 0\n            dice_scores.append(torch.tensor([dice_score(outputs_WT, ground_truths_WT),\n                                dice_score(outputs_TC, ground_truths_TC),\n                                dice_score(outputs_ET, ground_truths_ET)]))\n    del images, ground_truths, outputs, outputs_WT, outputs_TC, outputs_ET, ground_truths_WT, ground_truths_TC, ground_truths_ET\n    gc.collect()\n    loss_avg /= len(loader)\n    model.train()\n    dice_scores = torch.stack(dice_scores)\n    return loss_avg, dice_scores.mean(0)\n\ndef output_metrics(loss_avg, dice_scores, phase):\n    scores = dice_scores\n    print(phase)\n    print(\"Average loss: {}\".format(loss_avg))\n    print(\"Dice scores - peritumoral edema:{}, necrotic core:{}, enhancing core:{}\".format(scores[0], scores[1], scores[2]))","metadata":{"papermill":{"duration":0.046562,"end_time":"2021-05-02T19:27:42.889113","exception":false,"start_time":"2021-05-02T19:27:42.842551","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T22:37:13.990785Z","iopub.execute_input":"2021-06-10T22:37:13.991364Z","iopub.status.idle":"2021-06-10T22:37:14.018821Z","shell.execute_reply.started":"2021-06-10T22:37:13.991244Z","shell.execute_reply":"2021-06-10T22:37:14.017332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Iscrtavanje grafa gubitka","metadata":{}},{"cell_type":"code","source":"def plot_progress(data):\n    valid_loss = data['valid loss']\n    train_loss = data['train loss']\n\n    fig, ax = plt.subplots(figsize=(16,8))\n    linewidth = 2\n    legend_size = 10\n    train_color = 'm'\n    val_color = 'c'\n\n    ax.set_title('Loss')\n    ax.plot(train_loss, marker='o', color=train_color,\n           linewidth=linewidth, linestyle='-', label='train')\n    ax.plot(valid_loss, marker='o', color=val_color,\n           linewidth=linewidth, linestyle='-', label='validation')\n    ax.legend(loc='upper right', fontsize=legend_size)\n\n    save_path = os.path.join('./', 'loss.png')\n    print('Plotting in: ', save_path)\n    plt.savefig(save_path)\n    return","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:37:14.020756Z","iopub.execute_input":"2021-06-10T22:37:14.021593Z","iopub.status.idle":"2021-06-10T22:37:14.035455Z","shell.execute_reply.started":"2021-06-10T22:37:14.021547Z","shell.execute_reply":"2021-06-10T22:37:14.034207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Treniranje i evaluacija modela","metadata":{"papermill":{"duration":0.024247,"end_time":"2021-05-02T19:32:36.880921","exception":false,"start_time":"2021-05-02T19:32:36.856674","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def trainNetwork():\n    global device\n    plot_data = {}\n    plot_data[\"train loss\"] = []\n    plot_data[\"valid loss\"] = []\n    plot_data[\"valid dice WT\"] = []\n    plot_data[\"valid dice NC\"] = []\n    plot_data[\"valid dice ET\"] = []\n    plot_data[\"lr\"] = []\n    \n    SAVE_DIR = \"./network.pt\"\n    \n    print(\"device:\", device)\n\n    net = SemSegModel(ResNet(ResidualBlock, [2,2,2,2]), 4).to(device = device)\n    \n    lossFunc = nn.CrossEntropyLoss(weight = torch.tensor([2,5,3,1]).float().to(device=device))\n    \n    optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay = weight_decay)\n    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n    \n    for e in range(num_epochs):\n        accLoss = 0.\n        net.train()\n        for i, data in enumerate(trainloader):\n            inputs, ground_truths = data\n            inputs = inputs.to(device = device)\n            ground_truths = ground_truths.to(device=device)\n            \n            outputs = net(inputs)\n            ground_truths = un_one_hot(split_segments(ground_truths, ground_truths.shape[0])).to(device=device)\n            loss = lossFunc(outputs, ground_truths)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            accLoss += loss.item()\n            \n            if i % 10 == 0:\n                print(\"Epoch: %d, Iteration: %5d, Loss: %.3f\" % ((e + 1), (i+1), (accLoss / (i + 1))))\n            del loss, outputs, inputs, ground_truths\n            gc.collect()\n            \n        plot_data[\"train loss\"].append(accLoss/(i+1))\n        val_loss, val_dice = evaluate(net, validloader, lossFunc)\n        output_metrics(val_loss, val_dice, \"Validation\")\n        plot_data[\"valid loss\"].append(val_loss)\n        plot_data[\"valid dice WT\"].append(val_dice[0])\n        plot_data[\"valid dice NC\"].append(val_dice[0])\n        plot_data[\"valid dice ET\"].append(val_dice[0])\n        plot_data[\"lr\"].append(scheduler.get_last_lr())\n        \n        gc.collect()\n        \n        scheduler.step()\n    \n    test_loss, test_dice = evaluate(net, testloader, lossFunc)\n    output_metrics(test_loss, test_dice, \"Test\")\n    \n    torch.save(net.state_dict(), SAVE_DIR)\n    \n    plot_data[\"test loss\"] = test_loss\n    plot_data[\"test dice\"] = test_dice\n    \n    try:\n        with open(\"./epoch_data.txt\", 'w') as f:\n            f.write(repr(plot_data))\n        plot_progress(plot_data)\n    except:\n        print(\"Oops\")\n    \n    return plot_data\n    \nepoch_data = trainNetwork()","metadata":{"papermill":{"duration":23067.746294,"end_time":"2021-05-03T01:57:04.651726","exception":false,"start_time":"2021-05-02T19:32:36.905432","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T22:37:14.039375Z","iopub.execute_input":"2021-06-10T22:37:14.040004Z"},"trusted":true},"execution_count":null,"outputs":[]}]}